\section{Instances of Functor, Applicative, Comonad}\label{sec:type_classes}

This section outlines the Functor, Applicative, and Comonad instances, proving that the functions defined on monadic streams to implement these satisfy the relevant laws. We will prove these with a mixture of equational reasoning (using Haskell), and categorical reasoning, in each case making particular assumptions about the underlying functor.

\subsection{Functor Instance}

To show that $\stream{M}$ is a functor whenever $M$ is, we have to define its behaviour on morphisms: if $f:A\rightarrow B$, then we must define how $f$ maps on monadic streams:
$$
\begin{array}{l}
\stream{M}\,f : \stream{M,A} \rightarrow \stream{M,B}\\
\stream{M}\,f\,(\mcons\,m) = \mcons\,(M\,(f\times \stream{M}\,f)\,m)
\end{array}
$$

This definition complies with the {\em guarded-by-constructors} discipline: the recursive call to $(\stream{M}\,f)$ is mapped to the recursive substreams by the functorial application of the functor $M\,(A \times -)$.
That is: $(\stream{M}\,f)$ will be recursively applied only at the recursive positions inside the shape of $m$.
There are also applications of $f$ to the first element (of type $A$) of the pairs in the $M$-position: this is non-recursive, and therefore not problematic.

The Haskell version of the functorial mapping uses a general stream transformer at the top level: \hcode{uncons} maps between \hcode{MonStr m a} and \hcode{MonStr m b} by mapping through $m$ a function on the components:

\begin{haskell}
uncons :: MonStr m a -> m (a, MonStr m a)
uncons (MCons m) = m

transformM :: Functor m => (a -> MonStr m a -> (b, MonStr m b)) ->
                                  MonStr m a -> MonStr m b
transformM f s = MCons $ fmap (\(h,t) -> f h t) (uncons s)

instance Functor m => Functor (MonStr m) where
   -- fmap :: (a -> b) -> MonStr m a -> MonStr m b
   fmap f = transformM (\a s -> (f a, fmap f s))
\end{haskell}

We can now prove that the functor laws are satisfied by $\stream{M}$: it's functorial mapping preserves identities and composition.
The proofs are straightforward applications of definitions and the functoriality of $M$ and $\times$, except for the use of coinduction;
we are allowed to invoke the laws themselves in their proofs, as long as we use them only in the direct recursive subterms of the $\mcons$ constructor, that is, in the {\em positions} for the container $M\,(A\times -)$.

\begin{lemma}\label{lemma:functor_id}
The identity functor law holds for monadic streams:
$$
\stream{M}\,\id_A = \id_{\stream{M,A}}
$$
\end{lemma}
\begin{proof}
We apply the left-hand side function to an $M$-monster in constructor form:
$$
\begin{array}{ll}
\stream{M}\,\id_A\,(\mcons\,m)\\
{}= \mcons\,(M\,(\id_A\times \stream{M}\,\id_A)\,m)
  & \mbox{by definition}\\
{}= \mcons\,(M\,(\id_A\times \id_{\stream{M,A}})\,m)
  & \mbox{by coinduction hypothesis}\\
{}= \mcons\,(M\,(\id_{A\times \stream{M,A}})\,m)
  & \mbox{by functoriality of }\times\\
{}= \mcons\,(\id_{M\,(A\times \stream{M,A})}\,m)
  & \mbox{by functoriality of }M\\
{}= \mcons\,m
\end{array}
$$
\end{proof}

\begin{lemma}\label{lemma:functor_comp}
The composition functor law holds for monadic streams:

If $f:A\rightarrow B$ and $g:B\rightarrow C$, then
$$
\stream{M}\,(g\comp f) = (\stream{M}\,g) \comp (\stream{M}\,f)
$$
\end{lemma}
\begin{proof}
Let's again apply the left-hand side function to an $M$-monster in constructor form:
$$
\begin{array}{ll}
\stream{M}\,(g\comp f)\,(\mcons\,m)\\
{}= \mcons\,(M\,((g\comp f)\times \stream{M}\,(g\comp f))\,m)
  & \mbox{by definition of }\stream{M}\mbox{ mapping}\\
{}= \mcons\,(M\,((g\comp f)\times (\stream{M}\,g) \comp (\stream{M}\,f))\,m)
  & \mbox{by coinduction hypothesis}\\ 
{}= \mcons\,(M\,((g\times \stream{M}\,g) \comp (f\times \stream{M}\,f))\,m)
  & \mbox{by functoriality of }\times\\ 
{}= \mcons\,((M\,(g\times \stream{M}\,g) \comp M\,(f\times \stream{M}\,f))\,m)
  & \mbox{by functoriality of }M\\ 
{}= \mcons\,(M\,(g\times \stream{M}\,g)\, (M\,(f\times \stream{M}\,f)\,m))
  & \mbox{by definition of composition}\\ 
{}= \stream{M}\,g\, (\mcons\,(M\,(f\times \stream{M}\,f)\,m))
  & \mbox{by definition of }\stream{M}\mbox{ mapping}\\ 
{}= \stream{M}\,g\, (\stream{M}\,f\,m)
  & \mbox{by definition of }\stream{M}\mbox{ mapping}\\ 
{}= ((\stream{M}\,g) \comp (\stream{M}\,f))\,m
  & \mbox{by definition of composition}
\end{array}
$$
\end{proof}

We can sum up these results by stating that the monster operator is a functor if the underlying ``monad'' is (remember that we are not actually assuming that $M$ is a monad yet, but just a type operator).

\begin{theorem}
If $M$ is a functor, $\stream{M}$ is also a functor.
\end{theorem}

\subsection{Applicative instance}

Applicative functors \cite{mcbride/paterson:2008}
extend the mapping operation by allowing function sequencing under the functor.
It has two methods: $\apure$, that injects single values into the functor, and $\appl$, that applies functions under the functor.

We assume that the type operator $M$ is an applicative functor, that is, it has methods:
$$
\begin{array}{l}
\apure : A \rightarrow M\,A\\
(\appl): M\,(A\rightarrow B) \rightarrow M\,A \rightarrow M\,B
\end{array}
$$
satisfying the applicative laws.

A typical use of applicative functors is to apply a function of many arguments to several applicative values.
If $g:A_0\rightarrow A_1 \rightarrow \cdots \rightarrow A_n \rightarrow B$ and $a_0:M\,A_0, a_1:M\,A_1, \ldots, a_n:M\,A_n$ \ccomm{I changed $a_n:M\,a_n$ to $a_n:M\,A_n$, and did the same for the others}, then:
$$
(\apure\,g) \appl a_0 \appl a_1 \appl \cdots \appl a_n : M\,B
$$

In particular, if $g$ is an infix binary operator $(\oplus) : A \rightarrow B \rightarrow C$, then we use the notation:
$$
a \alift{\oplus} b = (\apure\,(\oplus)) \appl a \appl b
$$

We will show that $\stream{M}$ is also applicative.
In order to define the methods, we need some auxiliary functions on applicative monsters.
First of all, a simplified version of $\mcons$ that appends a single value in front of a monster.
This in turn uses a similar operator for functors, which appends an $M$-action to the front of a monster
$$
\begin{array}{l}
(\fcons): M\,A \rightarrow \stream{M}\,A \rightarrow \stream{M}\,A\\
m \fcons \sigma = \mcons\, (M\,(\lambda a. \langle a, \sigma\rangle)\,m)\\
\,\\
(\acons): A \rightarrow \stream{M}\,A \rightarrow \stream{M}\,A\\
a \acons \sigma = (\apure\, a) \fcons \sigma
\end{array}
$$

In Haskell:
\begin{haskell}
(<::) :: Functor m => m a -> MonStr m a -> MonStr m a
ma <:: s = MCons (fmap (\a -> (a,s)) ma)

(<:) :: Applicative m => a -> MonStr m a -> MonStr m a
a <: s = pure a <:: s
\end{haskell}

The $\apure$ method for monsters then consists in repeating the same element forever:
$$
\begin{array}{l}
\apure : A \rightarrow \stream{M}\,A\\
\apure\,a = a \acons \apure\,a
\end{array}
$$

In order to define the function application method, we need several binary operations applying functions to arguments inside products, $M$-actions, and $M$-monsters.
They are mutually recursive:
$$
\begin{array}{l}
(\appl) : \stream{M}\,(A\rightarrow B) \rightarrow \stream{M}\,A \rightarrow \stream{M}\,B\\
(\mcons\,m_f) \appl (\mcons\,m_a)
= \mcons\,(m_f \alift{\pappl} m_a)\\
\qquad \where\;
\langle f,\phi\rangle \pappl \langle a,\sigma\rangle 
= \langle f\,a, \phi \appl \sigma \rangle
\end{array}
$$

This definition recursively applies $\appl$ indirectly in the second components of the arguments of the $\pappl$ operator.
This is lifted to $\alift{\pappl}$, which distributes down through the components of the applicative values $\phi$ and $\sigma$, and finally guarded by the constructor $\mcons$.
This guarantees the soundnes of the definition according to the {\em guardedness by constructors} criterion.

Here is the definition of the Applicative instance for monsters in Haskell:
\begin{haskell}
transformA :: Applicative m =>
               (a -> MonStr m a -> b -> MonStr m b -> (c, MonStr m c)) ->
               MonStr m a -> MonStr m b -> MonStr m c
transformA f as bs = MCons $ (\(a,as') (b,bs') -> f a as' b bs')
                               <$> uncons as <*> uncons bs

instance Applicative m => Applicative (MonStr m) where
  pure a = a <: pure a
  (<*>) = transformA (\f fs a as -> (f a, fs <*> as))  
\end{haskell}

An applicative instance must satisfy four general laws regulating the interaction of sequencing and pure values and requiring associativity of application.
Preliminary investigation and analysis of specific instances suggest that this is the case for monsters.
A full proof is one of the goals of future work.

\begin{conjecture}\label{lemma:appl_laws}
If $M$ is an applicative functor, $\stream{M}$ is also an applicative functor.
That is, the following laws are satisfied, for every $a:A$, $f:A\rightarrow B$, $\sigma_a:\stream{M}\,A$, $\sigma_f : \stream{M}\,(A\rightarrow B)$, $\sigma_g: \stream{M}\,(B\rightarrow C)$:
$$
\begin{array}{l}
(\apure\,\id) \appl \sigma_a = \sigma_a \\
\apure\,(f\,a) = (\apure\,f) \appl (\apure\,a) \\
\sigma_f \appl (\apure\,a) = (\apure\,(\lambda f. f\,a)) \appl \sigma_f \\
\sigma_g \appl (\sigma_f \appl \sigma_a) = (\sigma_g \alift{\comp} \sigma_f) \appl \sigma_a
\end{array}
$$
\end{conjecture}







\subsection{Comonad instance in Haskell}

Comonads (in cojoin form) are functors with two additional operations: $\eta$ that extracts an element from the functor, and $\mu$, that 'duplicates' a functor:
$$
\begin{array}{l}
\eta : W\,A \to A\\
\mu : W\,A \to W\,(W\,A)
\end{array}
$$

For this to constitute a comonad, these operations need to satisfy the comonad laws.

These operations are expressed in Haskell with the \hcode{extract} and \hcode{duplicate} functions of the \hcode{Comonad} type class:

\begin{haskell}
class Functor w => Comonad w where
  extract :: w a -> a
  duplicate :: w a -> w (w a)
\end{haskell}

We believe that monadic streams are comonads, when the underlying functor is a comonad. We call these comonadic streams.

In the following Haskell equations, the function \verb+head+ returns the first element of the first pair in the monster, wrapped in the underlying functor. We require the underlying functor to be a comonad, so that we can use \hcode{extract} to remove the functor wrapping.

We introduce \verb+mm(s)+ for the "monster matrix" formed by a monadic stream \verb+s+ - this is defined as a monster where the first element is \verb+s+, the second is the tail of \verb+s+, the third is the tail of the tail of \verb+s+, and so on.

The \verb+duplicate+ instance for comonadic streams forms this "monster matrix": \verb+duplicate s = mm(s)+

\subsubsection{Comonad law proof sketches}

Proof that \verb+extract . duplicate == id+

\begin{haskell}
extract . duplicate 
= \ms -> extract (MCons $ fmap (\(h,t) -> (ms, duplicate t)) 
	(uncons ms))
= \ms -> extract (head (MCons $ fmap (\(h,t) -> (ms, duplicate t)) 
	(uncons ms)))
= \ms -> ms 
= id
\end{haskell}

Sketch of proof that \verb+fmap extract . duplicate == id+

\begin{haskell}
fmap extract . duplicate = \ms -> fmap extract (MCons $ fmap (\(h,t) -> 
	(ms, duplicate t)) (uncons ms))
= \ms -> transformM (\a s -> (extract a, fmap extract s)) (MCons $ 
	fmap (\(h,t) -> (ms, duplicate t)) (uncons ms))
  [transformM definition]
= \ms -> MCons $ fmap (\(h,t) -> (extract h, fmap extract t)) 
	(fmap (\(h,t) -> (ms, duplicate t)) (uncons ms)) 
= \ms -> MCons $ fmap (\(h,t) -> (extract ms, fmap extract (duplicate t))) 
	(uncons ms)
  [coinductive hypothesis]
= \ms -> MCons $ fmap (\(h,t) -> ((extract . head) ms, id t)) (uncons ms)
  [(extract . head) ms = a, first element of ms]
= \ms -> MCons $ fmap (\(h,t) -> (a, id t)) (uncons ms)
  [a is defined as the first element of ms, so h = a under the fmap]
= \ms -> ms
= id
\end{haskell}
 
Proof that \verb+duplicate . duplicate == fmap duplicate . duplicate+

\begin{haskell}
duplicate . duplicate = \ms -> duplicate (duplicate ms)
= \ms -> MCons $ fmap (\(h,t) -> (duplicate ms, duplicate t)) $
	(fmap (\(h,t) -> (ms, duplicate t)) (uncons ms))
= \ms -> MCons $ fmap ((\(h,t) -> (duplicate ms, duplicate (duplicate t))) 
	(uncons ms)
                      
fmap duplicate . duplicate = \ms -> fmap duplicate (MCons $ fmap (\(h,t) -> 
	(ms, duplicate t)) (uncons ms))
= \ms -> transformM (\a s -> (duplicate a, fmap duplicate s)) $
	(MCons $ fmap (\(h,t) -> (ms, duplicate t)) (uncons ms))
= \ms -> MCons $ fmap (\(h,t) -> (\a s -> 
	(duplicate a, fmap duplicate s)) h t) (uncons (MCons $ fmap (\(h,t) -> 
	(ms, duplicate t)) (uncons ms)))
= \ms -> MCons $ fmap (\(h,t) -> (duplicate h, fmap duplicate t)) $ 
	(fmap (\(h,t) -> (ms, duplicate t)) (uncons ms))
= \ms -> MCons $ fmap (\(h,t) -> 
	(duplicate ms, fmap duplicate (duplicate t))) (uncons ms)
  [coinductive hypothesis]
= \ms -> MCons $ fmap ((\(h,t) -> 
	(duplicate ms, duplicate (duplicate t))) (uncons ms)
= duplicate . duplicate
\end{haskell}

The first proof definitely holds, and the third is very convincing. The second needs some work, as there may be assumptions made that aren't consistent with the underlying functor being a comonad. 

As with the applicative functor laws, we are looking to improve these proofs, and also to formulate them in more categorical terms.


%\section{Monad proof}
%\input{monster_article_monad_proof}

\subsection{Monad Counter-example}

It's natural to ask, given the chosen name for this data structure, whether monsters themselves are monads, at least when the underlying functor is a monad. This is also a natural question because pure streams (with no extra functor guarding the elements) are monads. However, this turns out not to be the case in general, and we suspect that more constraints need to be placed on the underlying monad for the monster itself to be a monad. Namely, we believe the underlying monad should be idempotent. \\

A monad (in join form) is a functor with two additional operations: $\mathsf{return}$ that trivially injects an element into the functor, and $\mathsf{join}$ that flattens a doubly nested monadic functor into a single layer: 
$$
\begin{array}{l}
\mathsf{return} : A \to M\,A\\
\mathsf{join} : M\,(M\,A) \to M\,A
\end{array}
$$

For this to constitute a monad, these operations need to satisfy the monad laws. 
In Haskell, a monad can be defined with the \hcode{Monad} type class:

\begin{haskell}
class Functor m => Monad m where
  return :: a -> m a
  join :: m (m a) -> m a
\end{haskell}

\emph{(Currently in Haskell, the monad type class requires the functor to already have an \hcode{Applicative} instance, and also is defined in terms of \hcode{(\ >>=\ )}, called bind. However, the definition shown here is equivalent, as if a functor is a monad it implies that it is applicative, and \hcode{(\ >>= f) = join . fmap f})}\\

It is clear that the only sensible implementation of \hcode{return} is the same as that of $\apure$ in the applicative functor definition. In Haskell, \hcode{return} is usually defined as \hcode{pure} from the \hcode{Applicative} type class.

An example of why monadic streams aren't monads in general uses State-monsters. The one defined here, when run, generates the stream of integers where the first two differ by $n$, the next two differ by $n+1$, and so on:
\begin{haskell}
fromStep :: Int -> FBMachine Int Int
fromStep n = MCons (state (\x -> ((x, fromStep (n+1)), x+n)))
\end{haskell}
\verb+FBMachine (Int,Int)+ is shorthand for \verb+MonStr (State (Int, Int))+, standing for 'feedback machine' (this meaning is explained in the examples section).

For monadic streams to be a monad, we have to be able to join, or flatten, a monster of monsters into a single monster, with respect to certain laws - this is done in Haskell using a function \verb+>>=+, pronounced 'bind'. We also need a function \verb+return+ which injects a pure value into a monster.

The only return operation that makes sense is to apply the return of the underlying monad, and then repeatedly nest this inside itself (the same as \verb+pure+ from the Applicative instance):
\begin{haskell}
return :: Monad m => a -> MonStr m a
return a = MCons $ fmap (\a -> (a, return a) (return a)
\end{haskell}
Bear in mind that the rightmost \verb+return a+ is of type \texttt{a $\to$ m a}, and the one nested inside the tuple is of type \texttt{Monad m $\Rightarrow$ a $\to$ MonStr m a}. \\

One of the monad laws any monad needs to obey is the left identity law, which states: 
\begin{haskell}
f :: a -> MonStr m b
a :: a
return a  >>=  f == f a
\end{haskell}
In Haskell this laws is defined in terms of the bind (\verb+>>=+) operation, but is equivalent to left identity law in the Kleisli category of a monad.

Another monad law that bind has to obey is the right identity:
\begin{haskell}
ma :: MonStr m a
ma  >>= return == ma
\end{haskell}

We use the fact that bind is defined in terms of join:
\begin{haskell}
(>>= f) = join . fmap f 
\end{haskell}
The \verb+join+ function in this case is of type \texttt{MonStr m (MonStr m a) $\to$ MonStr m a}, which can be thought of as flattening a monster of monsters down into a single monster. \\

To show that monsters are not a monad, we reason that for any possible definition of join, there are counter-examples where it doesn't satisfy either the left or right identity laws.\\

Running \verb+fromStep 1+ with \verb+1+ produces the stream:
\begin{haskell}
runFBStr (fromStep 1) 1 = 1 <: 2 <: 4 <: 7 <: 11 <: (*...*)
\end{haskell}

This is the stream where, starting at $1$ for example, you add $1$ to get $2$, then add $2$ to get $4$, then add $3$ to get $7$, and so on. The fact that each step in the stream can be thought of as an action of 'adding some number' is pivotal to the upcoming reasoning.\\

\verb+return 1+ produces a monster where every element is $1$, polymorphic in the monad. \verb+return+ is the left and right unit for composition of Kleisli arrows (in any correctly defined monad), so we ignore any actions produced by it as they have no effect. \\

Since \verb+>>=+ is defined in terms of \verb+fmap+ and \verb+join+, we can start by looking just at \verb+fmap fromStep (return 1)+, and come back to joining the monster of monsters later.
\begin{haskell}
fmap fromStep (return 1) = (fromStep 1) <: (fromStep 1) <: (*...*)
\end{haskell}

Now, to join (flatten) this into a single stream, we need to somehow reconstruct \verb+fromStep 1+. There are three ways to do this, WLOG:

\subsection{Case 1}
We cannot take just the head of each stream, because the first action in each is 'adding $1$' - we would end up with a stream where $1$ is added to the previous element to get the next, which wasn't the definition of \verb+fromStep 1+. This could be thought of as taking the 'horizontal'. 

\subsection{Case 2}
The other naive method, which would work in this case, is to just take the first element of the stream, which \emph{is} \verb+fromStep 1+. We call this taking the 'vertical', which doesn't work in general as shown by another example:

Consider the pure stream \verb+from n+, defined as
\begin{haskell}
from :: Monad m => Int -> MonStr m a 
from n = n <: (from (n+1))
\end{haskell}

\verb+from 0+ gives the stream of natural numbers.
\begin{haskell}
from 0 = 0 <: 1 <: 2 <: (*...*)
\end{haskell}

We now look a the monster of monsters defined with this in mind:
\begin{haskell}
fmap return (from 0) = (return 0) <: (return 1) <: (return 2) <: (*...*)
\end{haskell}

Considering the right identity monad law, it is clear in this case that the join operation cannot be defined as taking the first element of the monster, since \verb+return 0+ $\neq$ \verb+from 0+.

\subsection{Case 3}

The only other way, considering the two examples, is to take the diagonal. This is because, in the first example:
\begin{haskell}
fmap fromStep (return 1) = (fromStep 1) <: (fromStep 1) <: (*...*)
\end{haskell}
we need the first action of the first stream (adding $1$), the second action of the second stream (adding $2$), and so on, to recreate  \verb+fromStep 1+.

However, to get the second action of the second stream (an 'adding $2$' action), we also need to traverse the \emph {first} action of the second stream (another 'adding $1$' action), since each element in a monadic stream is guarded by the previous. This would result in the net effect of 'adding $3$', which isn't what we need to reconstruct the second action of \verb+fromStep 1+. \\

Any other, more convoluted way of trying to get the action of 'adding $2$' falls under either, or a combination of, these three cases, which can be seen by reasoning with the examples given. This shows that you cannot make monadic streams a monad, with the assumption that the underlying functor is a monad.

One solution to this is to only allow the underlying monad to be idempotent \cite{idempotent_monads}. This is a monad that 'squares to itself' - the join operation is a natural isomorphism. This isn't the case for the state monad for example: joining two 'add 3' actions results in an 'add 6' action.

Taking the diagonal would then work as a join operation for monadic streams, since with an idempotent monad, any order in which you join actions results in the same action. However, this is quite a strong constraint, and it would be better if there exists a weaker one.



