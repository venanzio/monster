\documentclass{article}

\usepackage{amssymb,amsmath,amsthm}
\usepackage{listings}
\usepackage{pstricks,pst-node}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\title{Programming with Monsters}
\author{Christopher Purdy and Venanzio Capretta}


\usepackage{notations}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}

%format 

\begin{document}
\bibliographystyle{plain}

\maketitle

\begin{abstract}
A monadic stream is a potentially infinite sequence of values in which a monadic action is triggered at each step before the generation of the next element.
A monadic action consists of some functional {\em container} inside which the unfolding of the stream takes place.
Examples of monadic actions are: executing some input/output interactions, cloning the process into several parallel computations, executing transformations on an underlying state, terminating the sequence.

We develop a library of definitions and universal combinators to program with monadic streams and we prove several mathematical results about their behaviour.

We define the type of Monadic Streams (MonStr), dependent on two arguments: the underlying monad and the type of elements of the sequence.
We use the following terminology: a monadic stream with underlying monad $m$ is called an $m$-monster.
The definition itself doesn't depend on the fact that the underlying operator is a monad.
We define it generally: some of the operators can be defined without any assumptions, some others only need the operator to be a Functor or Applicative Functor.
A different set of important combinators and theoretical results follow from the assumption that the operator is a Co-Monad rather than a Monad.

We instantiate the abstract MonStr type with several common monads (Maybe, List, State, IO) and show that we obtain well-known data structures.
Maybe-monsters are lazy lists, List-monsters are non-well-founded finitely branching trees, IO-monsters are interactive processes.
We prove equivalences between the traditional data types and their MonStr versions: the MonStr combinators instantiate to traditional operations.

Under some assumptions on the underlying functor/monad, we can prove that the MonStr type is also a Functor, and Applicative, or a Monad, giving us access to the special methods and notations of those type classes.
\end{abstract}

\section{Introduction}

Intuitive explanation of Monadic Streams and motivation.

\section{Monadic Streams}

A monadic stream is a sequence of values in which every stage is obtained by triggering a monadic action.
If $\sigma$ is such a stream, it will consist of an action for a certain monad $M$ that, when executed, will return a head (first element) and a tail (continuation of the stream).
This process can be continued in a non-well-founded way: streams constitute a coinductive type.

Formally the type of streams over a monad $M$ (let's call them $M$-monsters) is defined as:
$$
\begin{array}[t]{l}
\codata\;
\stream{M,A}:\set\\
\quad \mcons_M: M\,(A\times \stream{M,A})\rightarrow\stream{M,A}.
\end{array}
$$
Categorically, we can see this type as the final coalgebra of the functor $F\,X = M\,(A\times X)$. (This final coalgebra does not necessarily exists for every $M$, we will discuss this issue below.)


Instantiating $M$ with some of the most well-known monads leads to versions of known data types or to interesting new constructs.

If we instantiate $M$ with the identity monad, we obtain the type of pure streams.
Its usual definition is the following:
$$
\begin{array}[t]{l}
\codata\;
\stream{A}:\set\\
\quad (\scons): \nat\rightarrow \stream{\nat} \rightarrow\stream{\nat}.
\end{array}
$$
(The type of the constructor has been curried, as is common.)
An element of $\stream{A}$ is an infinite sequence of elements of $A$: $a_0 \scons a_1\scons a_2\scons \cdots$.

If we instantiate $M$ with the $\maybe$ monad we obtain the type $\stream{\maybe,A}$, equivalent to the type of lazy lists $\lst{A}$.
The $\maybe$ monad is a functor that adds an extra element to the argument type: $\maybe\,X \cong X+1$, so the single constructor $\mcons_\maybe: \maybe\,(A\times \stream{\maybe,A})\rightarrow\stream{\maybe,A}$ is equivalent to two constructors (for $\nothing$ and $\just$):
$$
\begin{array}[t]{l}
\codata\;
\lst{A}:\set\\
\quad (\scons): A\times \lst{A}\rightarrow\lst{A}\\
\quad \nil: \lst{A}.
\end{array}
$$
This means that an element of $\lst{A}$ is either an empty sequence $\nil$ or a non-empty sequence $a\scons \sigma$ where $a:A$ and $\sigma$ is recursively an element of $\lst{A}$.
Since this is a coinductive type, the constructor $(\scons)$ can be applied an infinite number of times.
Therefore $\lst{A}$ is the type of finite and infinite sequences.


\begin{vcomment}
Add example with $M = \lstsym $ (maybe introduce different notations for lazy and strict lists) and give some simple example of functions on monsters.
\end{vcomment}







It is important to make two observations about $M$.

First, $M$ does not need to be a monad for the definition to make sense.
In fact we will obtain several interesting results when $M$ satisfies weaker conditions, for example being just a functor.
So we will take $M$ to be any type operator (but see second observation) and we will explicitly state what properties we assume about it.
The most important instances are monads and it is convenient to use the facilities of monadic notation in programming and monad theory in reasoning.

The second observation is that it is not guaranteed in general that the $\codata$ type is well-defined.
Haskell will accept the definition when $M$ is any functor, but mathematically the type is well defined only when the functor $F\,X = M\,(A\times X)$ has a final coalgebra.

\begin{definition}\label{def:fincoal}
For any functor $F$, a {\em coalgebra} for $F$ is pair $\langle A,\alpha\rangle$ consisting of a type $A$ and a function $\alpha:A\rightarrow FA$.

We say that $\langle A,\alpha\rangle$ is a {\em final $F$-coalgebra} if, 
for every coalgebra $\langle X,\xi:X\rightarrow FX\rangle$, 
there is a unique coalgebra morphism between the the two coalgebras:
$f:\langle X,\xi\rangle \to \langle A,\alpha\rangle$.
Such a morphism is a function between the types that commutes with the coalgebra functions:
$$
\setlength\arraycolsep{30pt}
\begin{array}{cc} \ \\
\Rnode{a}{A} & \Rnode{fa}{FA} \\[30pt]
\Rnode{x}{X} & \Rnode{fx}{FX} \\ \ 
\end{array}
\psset{nodesep=5pt,arrows=->}
\ncline{a}{fa} \taput{\alpha}
\ncline{x}{fx} \tbput{\xi}
\ncline[linestyle=dotted]{x}{a}  \tlput{f}
\ncline{fx}{fa} \trput{Ff}
\qquad \alpha \circ f = Ff \circ \xi.
$$
\end{definition}

This definition means that we can define function into a coinductive type by giving a coalgebra.
For example, consider the data type of pure streams of natural numbers:
The type $\stream{A}$ is the final coalgebra of the functor $F(X) = A\times X$.

We can define a function into $\stream{A}$ by defining a coalgebra on the domain type.
For example, if we want to define a function that maps any natural number $n$ to the stream of numbers starting from $n$, $n\scons (n+1) \scons (n+2) \scons (n+3) \scons \cdots$, we can do it by the following coalgebra on $\nat$:
$$
\begin{array}{l}
\xi : \nat \rightarrow \nat\times \nat\\
\xi\,n = \langle n,n+1\rangle
\end{array}
$$
(Note that the target type of the coalgebra is $F(\nat) = \nat\times\nat$: The first $\nat$ is the parameter of the functor, while the second $\nat$ is the carrier of the coalgebra.)

In practical programming, we often let the coalgebra $\xi$ be implicit by directly defining the function $f$ recursively.
For example, the function above would be defined as:
$$
\begin{array}{l}
f: \nat \rightarrow \stream{\nat}\\
f\,n = n \scons f\,(n+1)
\end{array}
$$
Here the presence of the parameter $n$ and the argument of the recursive call $n+1$ implicitely give the coalgebra $n \mapsto \langle n, n+1\rangle$.

A class of functors for which the existence of a final coalgebra is guaranteed is that of {\em containers} \cite{AAG:2005}: they are a generalization of tree constructors in which any type can be used for branching; this leads to generalized types of non-well-founded trees.

A common good criterion for the acceptability of recursive definition with a final coalgebra as codomain is {\em guardedness by constructors}: it accepts any recursive equations in which the right-hand side is a term with a constructor on top and recursive calls occurring only as dierect arguments of that constructor.

See previous survey work \cite{capretta:2011} for an overview of the theory of final coalgebras, coinductive types, and corecursive definitions.

The definition of $\stream{M,A}$ is not meaningful for all $M$s, because the final coalgebra may not exist or not be unique.
A useful result is that a functor has a final coalgebra if it is a container \cite{AAG:2005}, and $F$ is a container if $M$ is \cite{capretta/fowler:2017}.
This is the case for all the instances that we consider (but there are well known counterexamples, like the powerset functor and the continuation functor).

Therefore we will silently assume that $M$ is a container and that the final coalgebra exists.
Cofinality means that we can define functions into the coalgebra by {\em corecursion} and we can prove properties of its elements by {\em coinduction}.

\section{The Coinduction Principle}

As inductive types come together with an {\em induction principle}, stating that we can prove statements about them by bottom-up recursion on their structure, coinductive types come with a {\em coinduction principle}, stating that we can prove equalities of their elements by top-down recursion on their structure.

Let us illustrate the idea with the example of pure streams:
Suppose that we want to prove that two stream $\sigma_0$ and $\sigma_1$ are equal.
Since streams are infinite sequences of elements, that will require proving equality of their entries in corresponding positions: if $\sigma_0 = a_0\scons a_1\scons a_2\scons \cdots$ and  $\sigma_1 = b_0\scons b_1\scons b_2\scons \cdots$, we must prove $a_0 = b_0$, $a_1 = b_1$, $a_2=b_2$, and so on (we assume equality on streams is extensional).
This is an infinite sequence of equalities to prove.
Clearly we cannot produce all the proofs explicitely.
However, the proof of equality can be seen itself as a stream of proofs of equalities of their elements.
We can use the same principle of guarded recursion to generate all the proofs: we recursively assume that we can prove the equality of the tail streams and we only need to give explicitely the equality of the heads.

More specifically, we often need to prove that two functions produce equal streams.
Suppose $f,g:X \rightarrow \stream{A}$ are the two functions, and they are both defined by guarded recursion (that is, defined by coalgebras):
$$
f\,x = h_f(x) \scons f\,(t_f(x)) \qquad g\,x = h_g(x) \scons f\,(t_g(x)).
$$
So $f$ is defined by the coalgebra $\langle h_f, t_f\rangle : X \rightarrow A\times X$ and $g$ is defined by the coalgebra $\langle h_g, t_g\rangle : X \rightarrow A\times X$.
We want to prove that the two functions are extensionally equal; let us give a name to the proof of that statement:
$$
H: \forall x:X, f\,x = g\,x.
$$
We can do this by proving directly that the heads must be equal, $p_h:h_f(x) = h_g(x)$, and we would like to invoke the {\em coinduction hypothesis} $H$ for the tails.
However, since the recursive calls are applied to potentially different elements of $X$ ($t_f(x)$ and $t_g(x)$), we can't apply $H$ directly.

We can get around this problem by generalizing our goal: instead of proving that the functions give the same result when applied to the same input, we aim for the stronger statement that they give the same result when applied to inputs related by a certain binary relation $\bisim$:
$$
H: \forall x y:X, x\bisim y \rightarrow f\,x = g\,x.
$$
If $\bisim$ is reflexive, this will imply our original goal.

Two other properties are necessary to make things work.
First, we need the equality of the heads of the output: if $x\bisim y$, then $h_f(x) = h_g(x)$.
Second, we need that $\bisim$ is preseved under taking the tail functions for $f$ and $g$: if $x\bisim y$, then $t_f(x) \bisim t_g(x)$; so that we can now apply $H$ to obtain that $f(t_f(x)) = g(t_g(x))$.

A relation $\bisim$ satisfying these two properties is called a {\em bisimulation} between the coalgebras $\langle h_f, t_f\rangle$ and $\langle h_g, t_g\rangle$.
The principle of coinduction states that bisimilar elements generate equal streams.

This notion can be generalized to coalgebras of any functor $F$.
To formulated it we need a notion of {\em lifting} of a relation by the functor $F$: if $\bisim$ is a relation on a type $X$, we want to lift it to a relation $\bslift{F}$ on $F\,X$.
A simple way to do it is to apply $F$ to the set-theoretic characterization of the relation: the set of all pairs that satisfy it: $R = \{ \langle x, y \rangle \mid x \bisim y\}$.
We can then use the functorial lift of the projections $\pi_0, \pi_1:R \rightarrow X$ to say that two elements $u,w:F\,X$ are related if there exists an element $s:F\,R$ such that $F\,\pi_0\,s = u$ and $F\,\pi_1\,s = w$.
This can be further generalized by allowing $R$ to be any type: a collection of {\em receipts} that certify that pairs of elements are related. 
In category theory, this notion is called a {\em span}.

\begin{definition}\label{def:span}
Let $A$ be a type; a {\em span} on $A$ is a triple $\langle R,r_1,r_2\rangle$ where $R$ is a type and $r_1, r_2$ are functions $R\rightarrow A$.
If $F$ is a functor, the {\em lifting of the span $R$ by $F$} is the span $\langle F\,R,F\,r_1,F\,r_2\rangle$ on $F\,A$.
\end{definition}

\begin{definition}\label{def:bisimulation}
Let $\langle A,\alpha \rangle$ be a coalgebra.
A span $\langle R,r_1,r_2\rangle$ is a {\em bisimulation} if there exists a morphism $\rho:R\rightarrow FR$ such that both $r_1$ and $r_2$ are coalgebra morphisms from $\langle R,\rho\rangle$ to $\langle A,\alpha\rangle$:
$$
\setlength\arraycolsep{30pt}
\begin{array}{cc} \ \\
\Rnode{r}{R} & \Rnode{a}{A} \\[30pt]
\Rnode{fr}{FR} & \Rnode{fa}{FA} \\ \ 
\end{array}
\psset{nodesep=5pt,arrows=->}
\ncline[offset=3pt]{r}{a} \taput{r_1}
\ncline[offset=-3pt]{r}{a} \tbput{r_2}
\ncline[offset=3pt]{fr}{fa} \taput{Fr_1}
\ncline[offset=-3pt]{fr}{fa} \tbput{Fr_2}
\ncline{r}{fr} \tlput{\rho}
\ncline{a}{fa} \trput{\alpha}
\quad
\begin{array}{l}
\alpha \circ r_1 = Fr_1 \circ \rho\\
\alpha \circ r_2 = Fr_2 \circ \rho.
\end{array}
$$
(The diagram here is used only to declare the type of the morphisms: we don't assume that it commutes.
The only equalities are the ones stated on the right.)
\end{definition}

The idea is that a relation is a bisimulation if, whenever two elements of $A$ are related by it, then their images through $\alpha$ are related by the lifting.
If you think of $\alpha$ as giving the structure of an element this says: if two elements are related, then they must have the same shape, with components in corresponding positions also related.
This notion of bisimulation was first introduced by Park \cite{park:1981} and Milner \cite{milner:1980} as a way of reasoning about processes.
Similar concepts were developed earlier in other fields and substantial previous work prepared the background for its appearance.
The survey article by Sangiorgi \cite{sangiorgi:2009} tells the history of the idea.  
Aczel \cite{aczel:1988} adopted it as the appropriate notion of equality for non-well-founded sets.
There are subtle differences between several notions of bisimulation that are not equivalent in full generality: recent work by Staton \cite{staton:2009} investigates their correlations.

On $\stream{A}$ a bisimulation is a binary relation $\bisim$ such that
$$
\forall s_1, s_2:\stream{A}. s_1 \bisim s_2 \Rightarrow
\hd{s_1} =\hd{s_2} \land
\tl{s_1}\bisim \tl{s_2}.
$$
Notice that $s_1\bisim s_2$ guarantees that corresponding elements in the infinite sequences defined by $s_1$ and $s_2$ are equal, that is, $s_1$ and $s_2$ are extensionally equal.
In fact, by repeatedly applying the above property, we have that $\heads{s_1}{n} = \heads{s_2}{n}$ for every $n$.

\begin{definition}\label{def:coinduction}
A coalgebra $\langle A,\alpha\rangle$ is said to satisfy the {\em coinduction principle} if every bisimulation $\langle R,r_1,r_2\rangle$ on it has $r_1=r_2$.
\end{definition}

Intuitively, the coinduction principle states that the elements of $A$ are completely characterised by their structure, which can be infinite.
There is a well-known connection between finality of a coalgebra and the coinduction principle.











































\section{Examples}

By instantiating monadic streams with different monads, you can form well-known data types, or variations of these. This has provided good insight into what kinds of operations are possible to perform on monadic streams in general.

For each one of these instances, we look at how they correspond to other related data structures. In the next section, we also investigate how a few generic functions on monadic streams act on each of these instances, with respect to the data structures they represent.

Many of these instances of monadic streams, and their related operations, are implemented in the attached library.

\subsection{Identity monad}

To recap from section 2, when instantiating $\stream{M,A}$ with the identity monad, we obtain the type of pure streams.
$$
\begin{array}[t]{l}
\codata\;
\stream{A}:\set\\
\quad (\scons): \nat\rightarrow \stream{\nat} \rightarrow\stream{\nat}
\end{array}
$$
All standard operations on streams can be implemented for this type.

Interestingly, pure streams are also comonads, and this is also true in general for any monadic stream where the underlying functor has a comonad structure. This is discussed at the end of this section, where we look at examples of comonadic streams.

\subsection{Maybe monad}

When the underlying functor is $\maybe$, we get the type of lazy lists

$$
\begin{array}[t]{l}
\codata\;
\lst{A}:\set\\
\quad (\scons): A\times \lst{A}\rightarrow\lst{A}\\
\quad \nil: \lst{A}
\end{array}
$$

This type is isomorphic to lazy lists, with every operation possible on lists also possible on Maybe-monsters. This isomorphism is witnessed by the two functions:

\begin{haskell}
fromL :: [a] -> MonStr Maybe a
fromL [] = Nothing
fromL (x:xs) = MCons (Just (x, fromL xs))

toList :: MonStr Maybe a -> [a]
toList (MCons Nothing) = []
toList (MCons Just (x, xs)) = x : (toList xs)
\end{haskell} 

It is clear from the definitions that these are the exact inverse of one another. 

This isomorphism has provided a useful benchmark to test different functions on generic monadic streams against. For each function in \verb+Data.List+ that has been generalised for monadic streams, we can test this generalised function with a Maybe-monster, and compare the output to that of the original function in \verb+Data.List+. For this, we use the QuickCheck library:

\begin{haskell}
prop_drop :: Property
prop_drop = forAll (genListMonStr >*< chooseInt (0,1000)) $
               \((l, ms), n) -> drop n l === toList (dropM n ms)
\end{haskell}

This function builds a QuickCheck property, which checks whether the drop function (removal of the first $n$ elements) on lists and Maybe-monsters work identically. This makes use of the isomorphism, allowing the types to be freely converted between in order to check equality. 

All standard (and many non-standard) operations on lists are implemented for generic monads, sometimes with the requirement that the functor is an instance of \verb+Foldable+ or \verb+Alternative+.

\subsection{List monad}

When instantiating with the list monad, we get the type of branch-labelled trees. These are a variation on the usual type of Rose trees, where the \emph{edges} of the trees contain values, not the nodes or leaves.

$$
\begin{array}[t]{l}
\codata\;
\mathsf{BLTree}(A):\set\\
\quad \mathsf{node} : \lst{A \times \mathsf{BLTree}(A)} \rightarrow \mathsf{BLTree}(A)\\
\quad \mathsf{leaf} : \mathsf{BLTree}(A)
\end{array}
$$

Branch-labelled trees with one extra 'root' element are isomorphic to Rose trees, as shown below - this corresponds to the idea that every node in a tree has a unique branch from its parent, with the exception of the root node, which has no parent.

\begin{haskell}
data RoseTree a = RNode a [RoseTree a]

phi :: (a, MonStr [] a) -> RoseTree a
phi (a , MCons ts) = RNode a (fmap phi ts)

psi :: RoseTree a -> (a, MonStr [] a)
psi (RNode a ts) = (a , MCons (fmap psi ts))
\end{haskell}

Branch-labelled trees in general are more useful for applications where you care only about \emph{paths} down a tree, or \emph{transitions} between states rather than the states themselves. 

As an example, we could either model a game of noughts and crosses as a Rose tree where each node stores a game state, or with a List-monster where each branch stores the move taken. It is clear from this that you can have an empty branch-labelled tree (a single node with no branches/moves), but not an empty Rose tree.

Another example is probability trees - the branches represent choices, and the labels the probabilities of those choices occurring. This corresponds nicely to the non-determinism semantics of the list monad. Included in the demonstration code are some operations on and examples of List-monsters, with this concept in mind. \\

Some standard operations on trees don't work for this variation, but traversal operations return the branch labels in the expected order, with the convention of traversing left-to-right. 

\subsection{Reader monad}

Yet another interesting type is that of Reader-monsters, which weakly correspond to the type of Mealy machines. 

These are a type of finite state transducer, with a set of states $S$, input alphabet $\Sigma$, output alphabet $\Delta$, an initial state $s_0 \in S$, and a transition function $\delta : S \times \Sigma \to S \times \Delta$. They are typically used to model computations where the outputs depend on both the internal state and an input.

$$
\begin{array}[t]{l}
\mathbf{record}\;
\mathsf{Mealy}(S,\Sigma,\Delta):\set\\
\quad \delta : S \times \Sigma \to S \times \Delta \\
\quad s_0 : S
\end{array}
$$

There is a close correspondence (maybe an equivalence?) between these two types, indicated by these natural transformations:
\begin{haskell}
data StateFunc i o = SF { getSF :: i -> (StateFunc i o, o) }

mealyToMonStr :: Mealy s i o -> SMStr i o
mealyToMonStr (Mealy s tf) = MCons (\e -> let (s', a) = tf (s, e) 
	in (a, mealyToMonStr (Mealy s' tf)))

monStrToMealy :: SMStr i o -> Mealy (StateFunc i o) i o
monStrToMealy (MCons f) = Mealy (aux f) (\(g, e) -> (getSF g) e)
	where 
		aux :: (e -> (a, MonStr ((->) e) a)) -> StateFunc e a
		aux f = SF (\e -> let (a, g) = f e in (aux (uncons g), a))
\end{haskell}

The functions produce extensionally equivalent Mealy machines, which when given the same inputs, produce the same outputs. The Reader-monster can be thought of as a Mealy machine where each state \emph{is} the transition function (or rather the part of the transition function corresponding to the state of the implied Mealy machine). \\

Another type that Reader-monsters correspond to, when the domain of the reader arrows are pairs of time values and another input type, is the type of signal functions as presented in \cite{frp_refactored}. Perez et al. discuss this correspondence, and in-fact show that monadic streams in general can play a useful role in Functional Reactive Programming (FRP), by modelling streams of inputs into reactive systems. This role, and other applications, are discussed further in section 7.
                             
\subsection{State monad}

Instantiation of monsters with the state monad gives a type that is similar again to that of Mealy machines, but supplies it's own input to each computation (after the first) - you can either 'restart' the machine by supplying a fresh state, or let it run with the states that it produces itself. This could be seen as a Mealy machine with feedback, one who's transition function returns a new input. We refer to this as a feedback machine ($\mathsf{FBMachine}$ for short):

$$
\begin{array}[t]{l}
\mathbf{record}\;
\mathsf{FBMachine}(S,\Sigma,\Delta):\set\\
\quad \delta : S \times \Sigma \to S \times \Sigma \times \Delta \\
\quad s_0 : S
\end{array}
$$

Another way to think about a State-monster is a delayed unfold of a pure stream of values. Defining a State-monster amounts to defining a function that takes state, and returns a value, a new state, and a continuation function (another State-monster). This is essentially a specification for an operation that, when given an initial state, will calculate an infinite stream of values - quite similar to the unfold function, also called the stream \emph{co-iterator}.

The key difference is that the State-monster can contain many \emph{different} functions that produce new states and outputs. This gives much more control over the output values, allowing each function to define what the next function should be, given an input state, making them analogous to state machines.

Most of what can be said about State-monsters is said in the reader monad section, since the type of feedback machines $\mathsf{FBMachine}(S,\Sigma,\Delta)$ is equivalent to $\mathsf{Mealy}(S,\Sigma,\Sigma \times \Delta)$. The main difference is in how the stream is traversed - the state monad's join operation threads the state though each nested computation, whereas a Reader-monster can't do this with the reader monad operations alone.

\subsection{IO monad}

A monadic stream of IO actions corresponds to a non-terminating process. Due to the nature of the IO monad, these operate much differently to other monsters.

One interesting property is that an IO-monster is the only kind of non-well founded (infinite) monster, as far as we know, where collapsing the whole stream into one monadic action is possible and meaningful. This is done using the \verb+runProcess+ family of functions:

\begin{haskell}
runProcess :: Process a -> IO [a]
runProcess (MCons s) = do (a,s') <- s
                          as     <- runProcess s'
                          return (a:as)
\end{haskell}

This is because IO actions can continuously interact with the outside world, and call other functions, \emph{without having to terminate}. The IO action produced by collapsing an IO-monster is the sequence of all IO operations in the stream. \\

Given a process (an IO-monster), you may want to use some of the output values in another computation. To get these values, you need to run the IO actions in the monster. However, since the first action will never return (as all of the subsequent actions are nested inside), this is not possible with strict IO. Instead, you have to use lazy IO, so that only the values that are calculated inside the IO-monster are immediately available. This requires \verb+unsafeInterleaveIO+, which defers execution of IO actions until their result is needed. 

In these functions, we are trying to output the first value computed by running a process \verb+proc0+:

\begin{haskell}
proc0 :: Process a

run0 :: IO ()
run0 = do as <- runProcess proc0
          putStrLn $ show (as !! 0)

run1 :: IO ()
run1 = do as <- unsafeRunProcess proc0
          putStrLn $ show (as !! 0)
\end{haskell}

Here, \verb+run0+ will run the process \verb+proc0+ and the code to show the first element will never be reached since \verb+runProcess proc0+ doesn't terminate. 

In \verb+run1+, we use \verb+unsafeRunProcess+, which is defined as:

\begin{haskell}
unsafeRunProcess :: Process a -> IO [a]
unsafeRunProcess (MCons s) = 
	do (a,s') <- s
	   as     <- unsafeInterleaveIO (unsafeRunProcess s')
	   return (a:as)
\end{haskell}

Each of the IO actions in the process are run with \verb+unsafeInterleaveIO+, allowing the process to return 'early', and the \verb+putStrLn $ show (as !! 0)+ statement to be reached. This then forces the first IO action in the process to run, since the first element of \verb+as+ is needed (\verb+as !! 0+ returns the first element of the accumulated values from running the process).

Using \verb+unsafeInterleaveIO+ introduces concurrency problems into otherwise relatively pure Haskell programs. For simple IO-monsters, that just output values to the screen, this is not a big issue, but more complicated ones may cause problems (for example ones that read and write to a file). \\

This problem informed the development of a more general operation on monadic streams, \verb+interleaveReadM+, which allows for interleaving two monsters, where each element in the second depends on elements in the first.

\begin{haskell}
interleaveReadM :: Monad m => MonStr m a -> MonStr (ReaderT a m) b 
															-> MonStr m b
interleaveReadM (MCons ma) (MCons f) = MCons $ 
	do (a, ma') <- ma
	   (b, f')  <- runReaderT f a 
	   return (b, interleaveReadM ma' f')
\end{haskell}

This is useful for sequencing IO-monsters, without resorting to possibly unsafe methods. It relies on the \verb+ReaderT+ monad transformer, monad transformers being something we haven't touched on in this paper. They are essentially a method of stacking different monads to combine their effects. Here, we are using the \verb+ReaderT a IO+ monad, which represents an IO action that is computed from some environment of type \verb+a+. 

Shown below is an example of combining two processes with this operation.

\begin{haskell}
inputProc :: Process Char
inputProc = MCons $ do c <- getChar
                       return (c, inputProc)
                       
outputProc :: Show a => MonStr (ReaderT a IO) ()
outputProc = MCons $ do a <- ask 
                        liftIO $ putStrLn (show a)
                        return ((), outputProc)

testProc :: IO ()
testProc = runVoidProcess (interleaveReadM inputProc outputProc)
\end{haskell}

Using this function lets you define dependant processes separately and then combine them afterwards. 

Variations on this function give lots of options for modifying processes. For example, we could write a version of \verb+run1+ from earlier using \verb+insertActReadM+, which inserts a 'dependent' monadic action at a given index.

\begin{haskell}
run1 :: IO [Int]
run1 = runProcess (insertActReadM 0 (\a -> 
	do putStrLn (show a); 
	   return ()) proc0)
\end{haskell}

This won't terminate, but prints the first element of the IO-monster like we wanted. 

To stop the process (without resorting to \verb+unsafeInterleaveIO+), we modify \verb+runProcess+ so that it stops unfolding the IO-monster when a given predicate is true:

\begin{haskell}
stopAtPred :: (a -> Bool) -> Process a -> IO a
stopAtPred p (MCons s) = do (a, s') <- s
                            if p a then (return a) else (stopAtPred p s')
\end{haskell}
 
As a side-note, \verb+runProcess+ can now be written as:
 
 \begin{haskell}
runProcess :: Process a -> IO a
runProcess = stopAtPred (\_ -> False)
\end{haskell}

It is now possible to terminate the process given a particular predicate. It is also possible to terminate a process at a particular index in the IO-monster, thus giving exactly the behaviour we wanted for \verb+run1+, without the unsafe IO (this is included in the example code, as it is a bit more complicated). \\

As demonstrated, IO-monsters are tricky, but provide an interesting intensional way of modelling processes, allowing them to be modified on the fly.
 
\section{General functions and operations}

In light of the interpretations of various types of monadic stream, some of the general operations defined have interesting uses in each context presented. 

Consider the takeM function


\section{Instances of Functor, Applicative, Monad}

Show that monadic streams are an instance of these three classes, under some assumptions about the underlying monad.

So far we determined that we think that MonStr satisfies the monad laws if the underlying monad is commutative and idempotent \cite{idempotent_monads}. (Proof?)

What about Applicative?

Give conterexamples where it isn't a monad (using State).

Also examples when it is a monad even if the underlying monad is not commutative and idempotent: with List as underlying monad we obtain Trees, which are a monad. Can this be generalized.

\subsection{Functor Instance}

To show that $\stream{M}$ is a functor whenever $M$ is, we have to define its behaviour on morphisms: if $f:A\rightarrow B$, then we must define how $f$ maps on monadic streams:
$$
\begin{array}{l}
\stream{M}\,f : \stream{M,A} \rightarrow \stream{M,B}\\
\stream{M}\,f\,(\mcons\,m) = \mcons\,(M\,(f\times \stream{M}\,f)\,m)
\end{array}
$$

This definition doesn't strictly comply with the {\em guarded-by-constructors} discipline: the recursive call to $(\stream{M}\,f)$ doesn't occur directly as an argument to the constructor $\mcons$, but it is below the $\times$ operator and an application of $M$.
However, this is just the application of the functor $M\,(- \times -)$ which defines the final coalgebras. In other words, this is actually how we determine the positions in the container tree.
\begin{vcomment}
	This is very badly explained: needs more extensive clarification.
\end{vcomment}
















\subsection{Functor instance in Haskell}

\begin{haskell}
unwrapMS :: MonStr m a -> m (a, MonStr m a)
unwrapMS (MCons m) = m

transformMS :: Functor m => (a -> MonStr m a -> (b, MonStr m b)) ->
                                  MonStr m a -> MonStr m b
transformMS f s = MCons $ fmap (\(h,t) -> f h t) (unwrapMS s)

instance Functor m => Functor (MonStr m) where
   -- fmap :: (a -> b) -> MonStr m a -> MonStr m b
   fmap f = transformMS (\a s -> (f a, fmap f s))
\end{haskell}

\subsubsection{Functor proof}

Proof that \hcode{fmap id == id}

\begin{haskell}
fmap id = transformMS (\a s -> id a, fmap id s)
-- definition of `transformMS`
= \s -> MCons $ fmap (\(h,t) -> (\a s -> (id a, fmap id s)) h t) (unwrapMS s)
-- application of `(\a s -> (id a, fmap id s))` to `h t`
= \s -> MCons $ fmap (\(h,t) -> (id h, fmap id t)) (unwrapMS s)
-- take as assumption that `fmap id t == id t` - coinductive hypothesis?
= \s -> MCons $ fmap (\(h,t) -> (id h, id t)) (unwrapMS s)
-- definition of `id`
= \s -> MCons $ fmap (\(h,t) -> (h,t)) (unwrapMS s)
-- definition of `id`
= \s -> MCons $ fmap id (unwrapMS s)
-- `unwrapMS` returns an element in a functor, so `fmap id == id` in this case
= \s -> MCons $ id (unwrapMS s)
-- application of `id`
= \s -> MCons $ (unwrapMS s)
-- MCons and unwrapMS are inverses
= \s -> id s 
= id
\end{haskell}

Proof that \verb+fmap (f . g) == (fmap f) . (fmap g)+

To simplify notation in the proof, we introduce the notation \verb+tr(f)+ for the expression \verb+\(h,t) -> (f h, fmap f t)+.

We're going to use the fact that the underlying operator \verb+m+ is a functor and so satisfies the functor laws, and also that the pairing operator \verb+(,)+ is functorial in both arguments.

By definition of \verb+fmap+ for monsters, we have that
\begin{haskell}
fmap f s = transformMS (\ a s -> (f a, fmap f s)) s
         = MCons $ fmap (\ (h,t) -> (\a s -> (f a, fmap f s) h t)) (unwrapMS s)
         = MCons $ fmap (\ (h,t) -> (f h, fmap f t)) (unwrapMS s)
         = MCons $ fmap tr(f) (unwrapMS s)
\end{haskell}

We also use the fact that the monster constructor \verb+MCons+ and the function \verb+unwrapMS+ that removes it are inverse of each other.

\begin{haskell}
fmap (f . g) = transformMS (\a s -> (f . g) a, fmap (f . g) s)
-- definition of `transformMS`
= \s -> MCons $ fmap (\(h,t) -> (\a s -> ((f . g) a, fmap (f . g) s)) h t) (unwrapMS s)
-- application of `(\a s -> ((f . g) a, fmap (f . g) s))` to `h t`
= \s -> MCons $ fmap (\(h,t) -> ((f . g) h, fmap (f . g) t)) (unwrapMS s)`
-- take as assumption that `fmap (f . g) t == (fmap f . fmap g) t` - coinductive hypothesis?
= \s -> MCons $ fmap (\(h,t) -> ((f . g) h, (fmap f . fmap g) t)) (unwrapMS s)
-- By fuctoriality of pairing in both components
= \s -> MCons $ fmap (tr(f) . tr(g)) (unwrapMS s)
-- By functoriality of the underlying operator m
= \s -> MCons $ (fmap tr(f)) . (fmap tr(g)) (unwrapMS s)
-- MCons and unwrapMS are inverses
= \s -> MCons $ fmap tr(f)) $ unwrapMS $ MCons $ fmap tr(g) (unwrapMS s)
= \s -> MCons $ fmap tr(f)) $ unwrapMS $ fmap g s
= \s -> fmap f $ fmap g s
= fmap f . fmap g
\end{haskell}

\subsection{Comonad instance in Haskell}

\begin{haskell}
instance Comonad w => Comonad (MonStr w) where
  --extract :: MonStr w a -> a
  extract = extract . headMS
  
  --duplicate :: MonStr w a -> MonStr w (MonStr w a)
  duplicate s = MCons $ fmap (\(h,t) -> (s, duplicate t)) (unwrapMS s)
\end{haskell}

\verb+headMS+ returns the first element of the first pair in the monster, wrapped in the underlying functor. Since this functor is required to be a comonad, extract can be used to return the wrapped values inside.

We introduce \verb+mm(s)+ for the "monster matrix" formed by a monadic stream \verb+s+ - this is defined as a monster where the first element is \verb+s+, the second is the tail of \verb+s+, the third is the tail of the tail of \verb+s+, and so on.

The \verb+duplicate+ instance for monadic streams forms this "monster matrix": \verb+duplicate s = mm(s)+

\subsubsection{Comonad law proofs}

Proof that \verb+extract . duplicate == id+

\begin{haskell}
extract . duplicate 
= \ms -> extract (MCons $ fmap (\(h,t) -> (ms, duplicate t)) 
	(unwrapMS ms))
= \ms -> extract (headMS (MCons $ fmap (\(h,t) -> (ms, duplicate t)) 
	(unwrapMS ms)))
= \ms -> ms 
= id
\end{haskell}

Proof that \verb+fmap extract . duplicate == id+

\begin{haskell}
fmap extract . duplicate = \ms -> fmap extract (MCons $ fmap (\(h,t) -> 
	(ms, duplicate t)) (unwrapMS ms))
= \ms -> transformMS (\a s -> (extract a, fmap extract s)) (MCons $ 
	fmap (\(h,t) -> (ms, duplicate t)) (unwrapMS ms))
  [transformMS definition]
= \ms -> MCons $ fmap (\(h,t) -> (extract h, fmap extract t)) 
	(fmap (\(h,t) -> (ms, duplicate t)) (unwrapMS ms)) 
= \ms -> MCons $ fmap (\(h,t) -> (extract ms, fmap extract (duplicate t))) 
	(unwrapMS ms)
  [coinductive hypothesis]
= \ms -> MCons $ fmap (\(h,t) -> ((extract . headMS) ms, id t)) (unwrapMS ms)
  [(extract . headMS) ms = a, first element of ms]
= \ms -> MCons $ fmap (\(h,t) -> (a, id t)) (unwrapMS ms)
  [a is defined as the first element of ms, so h = a under the fmap]
= \ms -> ms
= id
\end{haskell}

Proof that \verb+duplicate . duplicate == fmap duplicate . duplicate+

\begin{haskell}
duplicate . duplicate = \ms -> duplicate (duplicate ms)
= \ms -> MCons $ fmap (\(h,t) -> (duplicate ms, duplicate t)) $
	(fmap (\(h,t) -> (ms, duplicate t)) (unwrapMS ms))
= \ms -> MCons $ fmap ((\(h,t) -> (duplicate ms, duplicate (duplicate t))) 
	(unwrapMS ms)
                      
fmap duplicate . duplicate = \ms -> fmap duplicate (MCons $ fmap (\(h,t) -> 
	(ms, duplicate t)) (unwrapMS ms))
= \ms -> transformMS (\a s -> (duplicate a, fmap duplicate s)) $
	(MCons $ fmap (\(h,t) -> (ms, duplicate t)) (unwrapMS ms))
= \ms -> MCons $ fmap (\(h,t) -> (\a s -> 
	(duplicate a, fmap duplicate s)) h t) (unwrapMS (MCons $ fmap (\(h,t) -> 
	(ms, duplicate t)) (unwrapMS ms)))
= \ms -> MCons $ fmap (\(h,t) -> (duplicate h, fmap duplicate t)) $ 
	(fmap (\(h,t) -> (ms, duplicate t)) (unwrapMS ms))
= \ms -> MCons $ fmap (\(h,t) -> 
	(duplicate ms, fmap duplicate (duplicate t))) (unwrapMS ms)
  [coinductive hypothesis]
= \ms -> MCons $ fmap ((\(h,t) -> 
	(duplicate ms, duplicate (duplicate t))) (unwrapMS ms)
= duplicate . duplicate
\end{haskell}


\section{Monad proof}

\begin{definition}\label{def:monad}
For an endofunctor $T$ on a category $C$, a {\em monad} is a triple $(T, \mu, \eta)$, of $T$ and two natural transformations $\mu : TT \Rightarrow T$ and $\eta : 1_C \Rightarrow T$ ($1_C$ being the identity functor on $C$) such that these diagrams commute:
$$
\setlength\arraycolsep{30pt}
\begin{array}{cc} \ \\
\Rnode{t1}{TA} & \Rnode{tt}{TTA} \\[30pt]
 & \Rnode{t2}{TA} \\ \ 
\end{array}
\psset{nodesep=5pt,arrows=->}
\ncline{t1}{tt} \taput{\eta_{TA}}
\ncline{tt}{t2} \trput{\mu}
\ncline{t1}{t2} \tbput{id_{TA}}
\qquad 
\setlength\arraycolsep{30pt}
\begin{array}{cc} \ \\
\Rnode{t1}{TA} & \Rnode{tt}{TTA} \\[30pt]
 & \Rnode{t2}{TA} \\ \ 
\end{array}
\psset{nodesep=5pt,arrows=->}
\ncline{t1}{tt} \taput{T\eta_{A}}
\ncline{tt}{t2} \trput{\mu}
\ncline{t1}{t2} \tbput{id_{TA}}
$$
$$
\setlength\arraycolsep{30pt}
\begin{array}{cc} \ \\
\Rnode{ttt}{TTTA} & \Rnode{tt1}{TTA} \\[30pt]
\Rnode{tt2}{TTA} & \Rnode{t}{TA} \\ \ 
\end{array}
\psset{nodesep=5pt,arrows=->}
\ncline{ttt}{tt1} \taput{\mu T}
\ncline{ttt}{tt2} \tlput{T\mu}
\ncline{tt2}{t} \tbput{\mu}
\ncline{tt1}{t} \trput{\mu}
$$
\end{definition}

For the functor $\stream{M}$ to be a monad, the monad laws (these commuting diagrams) need to be satisfied. However, assuming a monad $(\stream{M}, \mu, \eta)$, the left identity law (top-left diagram) does not hold in general, given an arbitrary underlying monad $M$. This is due to a correct $\mu$ not being constructible.\\

Here, subscripts of $\eta$ (and $\mu$) indicate their monad, and not components of the natural transformation - these are instead indicated using function application: $\eta_M(x)$ where $x : A$ implies that we are using the component of $\eta_M$ at $A$. \\

First, note that $\eta_{\stream{M}}$ has to be defined as:

$$\eta_{\stream{M}}(x) = mcons_M(\eta_M ((x, \eta_{\stream{M}}(x)))$$ 

There is one choice of $A$ for the first element of the pair, and only one way of lifting pairs into the inner monad $M$, which is by using $\eta_M$. The second element of the pair has to be a $\stream{M,A}$, and the only way to produce this here is by a recursive call.\\

We introduce the notation $\mathop{|}_M^i$ to denote a monadic stream constructor guarded by a monadic action $m_i : A \to MA$. Since each monadic action in $M$ may be different, $i$ is just a label to indicate when they are the same. That is, for any $i$, and a fixed $a : A$:
\begin{align*}
&s_0 : \stream{M,A} = \mathop{|}_M^i a, s_0\\
&s_1 : \stream{M,A} = \mathop{|}_M^i a, s_1\\
&s_0 = s_1
\end{align*}
If $i = \eta$, this monadic action is the monad $M$'s unit, the natural transformation $\eta_M$. For any other label $i$, the monadic action is arbitrary. If there is no label, whether the monadic actions are equal is not being considered.\\

\begin{proof}
If $(\stream{M}, \mu, \eta)$ is a monad, then it needs to obey the left identity law, which in this case is
$$
\mu_{\stream{M}}(\eta_{\stream{M}}(s)) = s
\qquad s : \stream{M,A}
$$
For an arbitrary steam $s$, defined as:
$$
s = \mathop{|}_M^0 a_0, \mathop{|}_M^1 a_1, \mathop{|}_M^2  a_2, \dots
$$
First we construct the $M$-monster of $M$-monsters (termed an $M$-matrix) as per this requirement:
\begin{align*}
&\eta_{\stream{M}}(s) : \stream{M,\stream{M,A}} = \mathop{|}_M^\eta s, \mathop{|}_M^\eta s, \mathop{|}_M^\eta s, \dots
\end{align*}
To take the diagonal, you have to take the first element of the first stream, the second of the second stream, and so on. The law says that $\eta_{\stream{M}}(s)$ needs to be somehow manipulated to form $s$. \\

We reason that $\mu_{\stream{M}}$ cannot be constructed by noting that there is no function out of a monad in general (no function $f : MA \to A$ for an arbitrary functor M over which a monad is defined), and then traversing the $M$-matrix to see which monadic actions \emph{necessarily} have to guard each element in the $M$-monster resulting from an application of $\mu_{\stream{M}}$ to $\eta_{\stream{M}}(s)$.\\

Observe that the first element of the first stream $a_0$ in $\eta_{\stream{M,A}}(s)$ is guarded by two monadic actions, one of them $\eta$, the other an arbitrary action, which we'll call $m_0 : MA$. By the left identity on the monad $M$, $\mu(\eta(m_0)) = m_0$, which indicates that in the absolute best case, the first element of the $M$-monster is guarded by $m_0$.
$$
\eta_{\stream{M}}(s) : \stream{M,\stream{M,A}} = \mathop{|}_M^\eta (\mathop{|}_M^0 \stackrel{\downarrow}{a_0}, \mathop{|}_M^1 a_1, \dots), \mathop{|}_M^\eta s, \dots
$$
Here, "best case" refers to the minimum sequence of monadic actions you have to evaluate to get to a certain element in the stream, removing $\eta$ actions where possible as they act as a unit for Kleisli composition.

Now that we know the first element of the resulting stream $a_0$ will be, in the best case, guarded by $m_0$, we look at what actions guard the second element $a_1$. WLOG, there are two ways to extract $a_1$, either by taking the second element of the second $M$-monster, or by taking the second element of the first $M$-monster. This gives two cases to consider.\\

\paragraph{Case 1:}

In the first case: if we take the second element of the second $M$-monster, then we have to consider what actions guard the element $a_1$ - these in order are $\eta, \eta, m_0, m_1$, obtained by traversing along $\eta_{\stream{M}}(s)$ to the second $s$, and then along $s$ to $a_1$. By using the left identity again, this could reduce in the best case to $m_0(m_1(a_1))$.

$$
\eta_{\stream{M}}(s) : \stream{M,\stream{M,A}} = \mathop{|}_M^\eta s, \mathop{|}_M^\eta (\mathop{|}_M^0 a_0, \mathop{|}_M^1 \stackrel{\downarrow}{a_1}, \dots), \dots
$$

Now realise that in $s$, the element $a_0$ comes before $a_1$. This means that any monadic actions guarding $a_0$, have to also guard $a_1$. Crucially, the $m_0$ actions that guard them \emph{are not the same}, one comes from the first $s$, and the other from the second $s$. This means $a_1$ has to be guarded by \emph{two copies} of $m_0$ - one from the $s$ in the first index of $\eta_{\stream{M}}(s)$ (which is required to guard $a_0$), and one from the second. 

As you cannot get rid of this extra monadic action in general, $s$ is not constructible from $\eta_{\stream{M}}(s)$ in this way. The problem with a duplicated monadic action can be demonstrated with the State monad - if you have an action that adds $3$ to an underlying state, duplicating this would produce an overall action of adding $6$.\\


\paragraph{Case 2:} 

The second case to consider was to take the second element from the first stream. To show this doesn't work generally either, we have to instead consider the right monad identity law. This states that:

$$
\mu_{\stream{M}}(\stream{M}\eta_{\stream{M}}(s)) = s
\qquad s : \stream{M,A}
$$

Where $\stream{M}\eta_{\stream{M}}$ lifts $\eta_{\stream{M}}$ to a function on monadic streams, which effectively applies it to each of the elements (this is our \verb+fmap+ definition from earlier). We redefine the $M$-matrix accordingly:
\begin{align*}
&a_i : A\\
&\eta_{\stream{M}}(a_i) : \stream{M,A} = \mathop{|}_M^\eta a_i, \mathop{|}_M^\eta a_i, \mathop{|}_M^\eta  a_i, \dots\\
&\stream{M}\eta_{\stream{M}}(s) : \stream{M,\stream{M,A}} = \mathop{|}_M^0 \eta_{\stream{M}}(a_0), \mathop{|}_M^1 \eta_{\stream{M}}(a_1), \mathop{|}_M^2 \eta_{\stream{M}}(a_2), \dots
\end{align*}

Using similar logic to before, you can show that $a_0$ is guarded by at least $m_0$. However, if you pick the second element from the second stream, you get another $a_0$. The $\mu_{\stream{M,A}}$ operation has to cross over to the next 'sub-monster' $\eta_{\stream{M}}(a_1)$ to get the value $a_1$. This loops us back to the first case, where we show that doing this causes a duplication of monadic actions, considering the left identity. 

\end{proof}

\section{Applications}

\subsection{Examples}

Included with the library is a suite of tests showing the correspondence between functions on $Maybe$-monsters and functions on lazy lists defined in the Haskell standard library. 

This, alongside the reimplementation of Swiestra and Dijk's streams library, show that monadic streams can at least be used to generalise infinite and finite lists, using the identity and maybe monads respectively.

\begin{ccomment}
	Show a few applications and motivate the usefulness of Monadic Streams.
\end{ccomment}

\subsection{FRP applications}

One application of monadic streams is they can be seen as a generalised form of Yampa's \cite{yampa_arcade} signal functions. This is expanded on by Perez et al. in \cite{frp_refactored}. For example, one can implement the integral signal function using the Reader monad. First you define the type of monadic stream - we use the same names as for the same constructs in Yampa, to make the correspondence clear:

\begin{haskell}
type DTime = Double

type SignalFunc a b = MonStr ((->) (DTime, a)) b
\end{haskell}

\begin{haskell}
integral :: SignalFunc Double Double
integral = MCons integralAuxF
                 
integralAuxF :: (DTime, Double) -> (Double, SignalFunc DTime Double)
integralAuxF (_, a) = (0 , integralAux 0 a)
   where integralAux igrl a_prev = MCons (\(dt, a') -> 
   		(igrl' dt, integralAux (igrl' dt) a'))
      where igrl' dt' = igrl + (dt' * a_prev)
\end{haskell}

This can be generalised to any VectorSpace type.

\subsection{Relation to Dunai}

Dunai \cite{frp_refactored} is an FRP library implemented using monadic stream functions (MSFs). \\

This library we've developed allows for a wide variety of operations on monadic streams. Since the MSFs used in Dunai are a special case of monsters (instantiated with the Reader monad), most functions in our library could be incorporated into Dunai, giving a set of potentially useful generalised transformations for MSF networks. \\

For example, the \verb+insert+ combinator could be used to insert an arbitrary extra function into a monadic stream function.

Additionally, comonadic streams could be a useful extension to MSF based FRP - as seen in \cite{frp_refactored}, monadic streams can already model an input stream of data into a reactive program. Using comonadic streams however, you can model arbitrarily nested input streams. That is, inputs streams whose inputs are input streams. (this needs much more seeing to)

Comonadic streams (monadic streams where the underlying functor is a comonad) can be seen as a progressing timeline of an environment as it is operated on in some way. The comonad provides the notion of environment, and the stream embues it with a way of modelling its progression in time. These streams only model the notion of current and future values of the environment.

This could be useful to model systems which have some kind of self referential data store, or some kind of environment that can be used to modify itself.

\section{Conclusions}

Discussion of related literature.

Open problems.

Future Work.

\bibliography{monster_biblio.bib}

\end{document}
