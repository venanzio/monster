\section{Examples}

We now shift our view to concrete examples of monsters, and their potential uses.

By instantiating monadic streams with different monads, you can form well-known data types, or variations of these. This has provided good insight into what kinds of operations are possible to perform on monadic streams in general.

For each one of these instances, we look at how they correspond to other related data structures. In the next section, we also investigate how a few generic functions on monadic streams act on each of these instances, with respect to how they transform the data structures that these monsters represent.

All of these instances of monadic streams, and some of their related operations, are implemented in the attached library. For the most part we will consider operations that are polymorphic on the monad, as these are more novel in their behaviour.

\subsection{Identity monad}

To recap, when instantiating $\stream{M,A}$ with the identity monad, we obtain the type of pure streams.
$$
\begin{array}[t]{l}
\codata\;
\stream{A}:\set\\
\quad (\scons): \nat\rightarrow \stream{\nat} \rightarrow\stream{\nat}
\end{array}
$$
All standard operations on streams can be implemented for this type.

Interestingly, pure streams are also comonads, and this is true in general for any monadic stream where the underlying functor has a comonad structure. This is discussed at the end of this section, where we look at an example of a comonadic stream.

\subsection{Maybe monad}

When the underlying functor is $\maybe$, we get the type of lazy lists

$$
\begin{array}[t]{l}
\codata\;
\lst{A}:\set\\
\quad (\scons): A\times \lst{A}\rightarrow\lst{A}\\
\quad \nil: \lst{A}
\end{array}
$$

In Haskell, this type is implemented as the standard list type \hcode{[a]} for any type \hcode{a}. The $\nil$ constructor is just the empty list \hcode{[]}, and the $\scons$ constructor is represented by the operator \hcode{(:)}.
So the list $a_0\scons a_1\scons a_2\scons \nil$ is written in Haskell as either \hcode{a0:a1:a2:[]} or, with the nicer list notation, as \hcode{[a0,a1,a2]}.

The type $\stream{\maybe,A}$ of $\maybe$-monsters is isomorphic to the type $\lst{A}$ of lazy lists, with every operation possible on lists also possible on $\maybe$-monsters.
And indeed most operations can be generalized to polymorphic functions independent of the monad.

This isomorphism is witnessed by the two functions:
\begin{haskell}
fromL :: [a] -> MonStr Maybe a
fromL [] = Nothing
fromL (x:xs) = MCons (Just (x, fromL xs))

toList :: MonStr Maybe a -> [a]
toList (MCons Nothing) = []
toList (MCons Just (x, xs)) = x : (toList xs)
\end{haskell} 

It is clear from the definitions that these functions are the exact inverse of one another. 

This isomorphism has provided a useful benchmark  against which to test monad-polymorphic functions in our library. For each function in \verb+Data.List+ (a collection of list operations included in Haskell's standard library) that has been generalized to monadic streams, we can test this generalized function with a $\maybe$-monster, and compare the output to that of the corresponding function in \verb+Data.List+. For this, we use the QuickCheck \cite{quickcheck} library:

\begin{haskell}
prop_drop :: Property
prop_drop = forAll (genListMonStr >*< chooseInt (0,1000)) $
               \((l, ms), n) -> drop n l === toList (dropM n ms)
\end{haskell}

This function builds a QuickCheck property, which validates whether the drop function (removal of the first $n$ elements) on lists and $\maybe$-monsters are (extensionally) equivalent. This makes use of the isomorphism, allowing us to freely convert between these types in order to check for equality.

Included with the library is a whole suite of tests in this style, showing the correspondence between functions on $\maybe$-monsters and functions on lazy lists defined in the Haskell standard library.

All standard (and many non-standard) operations on lists are implemented for generic monads, sometimes with the requirement that the functor is an instance of \verb+Foldable+ or \verb+Alternative+.

\subsection{List monad}

When instantiating monsters with the list monad, we get the type of branch-labelled trees. These are a variation on the usual type of Rose trees, where in this case the \emph{edges}, rather than the nodes, of the trees are labelled by values.
$$
\begin{array}[t]{l}
\codata\;
\mathsf{BLTree}(A):\set\\
\quad \mathsf{node} : \lst{A \times \mathsf{BLTree}(A)} \rightarrow \mathsf{BLTree}(A)
\end{array}
$$
(Note that this type has an empty element $\mathsf{leaf} : \mathsf{BLTree}(A)$, obtained by using the empty list, $\mathsf{leaf} = \mathsf{node}\,\nil$.)

Branch-labelled trees with one extra `root' element are isomorphic to Rose trees, as shown below - this corresponds to the idea that every node in a tree has a unique branch from its parent, with the exception of the root node.

\begin{haskell}
data RoseTree a = RNode a [RoseTree a]

phi :: (a, MonStr [] a) -> RoseTree a
phi (a , MCons ts) = RNode a (fmap phi ts)

psi :: RoseTree a -> (a, MonStr [] a)
psi (RNode a ts) = (a , MCons (fmap psi ts))
\end{haskell}

Branch-labelled trees in general are more useful for applications where you care only about \emph{paths} down a tree, or \emph{transitions} between states rather than the states themselves. 

As an example, we could either model a game of noughts and crosses as a Rose tree where each node stores a game state, or with a $\lstsym$-monster where each branch stores the move taken. It is clear from this that you can have an empty branch-labelled tree (a single node with no branches/moves), but not an empty Rose tree (there is always an initial game state, even if no move has been made yet).

Another example is probability trees - the branches represent choices, and the labels are the probabilities of those choices occurring. This corresponds nicely to the non-determinism semantics of the list monad. Included in the demonstration code are some operations on and examples of $\lstsym$-monsters, with this concept in mind. \\

Some standard operations on trees don't work for this variation, but traversal operations return the branch labels in the expected order, with the convention of traversing left-to-right. 

\subsection{Reader monad}

Yet another interesting type is that of $\reader$-monsters.
The reader monad is simply an operator that generates the function type from a fixed input type $I$:
$$
\reader_I\,A = I \rightarrow A
$$
We can see $\reader_I$-monsters, elements of $\stream{\reader_I}\,A$, as trees with edges labelled by elements of $I$ and internal nodes labelled by elements of $A$.

$\reader$-monsters correspond to the type of Mealy machines. 

These are a type of finite state transducer, with a set of states $S$, input alphabet $\Sigma$, output alphabet $\Delta$, an initial state $s_0 \in S$, and a transition function $\delta : S \times \Sigma \to S \times \Delta$. They are typically used to model computations where the outputs depend on both the internal state and an input.

$$
\begin{array}[t]{l}
\mathbf{record}\;
\mathsf{Mealy}(S,\Sigma,\Delta):\set\\
\quad \delta : S \times \Sigma \to S \times \Delta \\
\quad s_0 : S
\end{array}
$$

There are mutual transformations between these two types.
These form an equivalence: when transforming a Mealy machine to a $\reader$-monad and then back to a Mealy machine, we don't get the same machine (the type of states has changed), but we get a Mealy machine with the same dynamic behaviour.

The direction from $\reader$-monsters to Mealy machines requires us to define the state type as a hierarchical recursive type from input to output.

\vcomm{It looks to me that \hcode{StateFunc i o} is essentially the same as \hcode{SMStr i o}, so can't we use just this?}


\begin{haskell}
type SMStr i o = MonStr ((->) i) o

data Mealy st inA outA = Mealy { initState :: st
                               , transf :: (st , inA) -> (st , outA)
                               }
   
data StateFunc i o = SF { getSF :: i -> (StateFunc i o, o) }

mealyToMonStr :: Mealy s i o -> SMStr i o
mealyToMonStr (Mealy s tf) = MCons (\e -> let (s', a) = tf (s, e) 
	in (a, mealyToMonStr (Mealy s' tf)))

monStrToMealy :: SMStr i o -> Mealy (StateFunc i o) i o
monStrToMealy (MCons f) = Mealy (aux f) (\(g, e) -> (getSF g) e)
	where 
		aux :: (e -> (a, MonStr ((->) e) a)) -> StateFunc e a
		aux f = SF (\e -> let (a, g) = f e in (aux (uncons g), a))
\end{haskell}

The functions produce extensionally equivalent Mealy machines, which when given the same inputs, produce the same outputs. A $\reader$-monster can be thought of as a Mealy machine where each state \emph{is} the transition function, more specifically the transition functions partially applied to each state of the implied Mealy machine. From now on we talk about $\reader$-monsters, finite state machines (FSM) and Mealy machines, all referring to the same data structure. \\

One function that works well with $\reader$-monsters is \verb+zipWithA+. This uses a binary operation to combine the elements of two monsters, producing a new monster with these combined elements. In the case of state machines, this amounts to having a pair of state machines where the same inputs are fed to both, and then the outputs are combined with some arbitrary function. Each FSM still changes state independently.

As a trivial example, you could have a FSM that outputs the maximum of the last $3$ inputs (where the set of inputs is ordered), and another that outputs the minimum. If you zip these together with a function that checks for equality, then you have a FSM that outputs 'true' when the last $3$ inputs were identical, and 'false' otherwise. This particular example is included in \verb+Examples.StateMachines+.

This function, and others such as \verb+interleaveReadM+ (discussed in section 5), give useful ways of building complex FSMs out of smaller ones. \\

Another type that $\reader$-monsters correspond to, when the domain of the reader arrows are pairs of time values and another input type, is the type of signal functions as presented in \cite{frp_refactored}. Perez et al. discuss this correspondence, and in-fact show that monadic streams in general can play a useful role in Functional Reactive Programming (FRP), by modelling streams of inputs into reactive systems. This role, and other applications, are discussed further in section 7.
                             
\subsection{State monad}

Instantiation of monsters with the state monad gives a type that is similar again to that of Mealy machines, but one that supplies its own input to each computation (after the first) - you can either `restart' the machine by supplying a fresh state, or let it run with the states that it produces itself. This could be seen as a Mealy machine with feedback, one who's transition function returns a new input. We refer to this as a feedback machine ($\mathsf{FBMachine}$ for short):
$$
\begin{array}[t]{l}
\mathbf{record}\;
\mathsf{FBMachine}(S,\Sigma,\Delta):\set\\
\quad \delta : S \times \Sigma \to S \times \Sigma \times \Delta \\
\quad s_0 : S
\end{array}
$$

Another way to think about a State-monster is an intensional unfold of a pure stream of values. Defining a State-monster amounts to defining a function that takes a state, and returns a value, a new state, and a continuation function (another State-monster). When the new state is continuously applied to the next continuation, this calculates an infinite stream of values from an initial 'seed' state - quite similar to the unfold function, also called the stream \emph{co-iterator}.

\begin{vcomment}
The explanation above is quite unclear.
It is not clear to me what the role of $\Sigma$ is: isn't this just the same as having $S\times \Sigma$ as state type?
 It needs to be clarified, but we have no time, so let's keep it for now. Below the definition of \hcode{FBMachine} doesn't correspond to the definition above: it is recursive instead of using a type of feedback values $\Sigma$, and doesn't have an initial state.
In general, both here and for the Mealy machines, we could remark that the machine type is a way of defining a coalgebra; the corresponding monster is then the {\em behaviour} of the coalgebra (see for example the coinductive treatment of finite state machines in Jacobs.
\end{vcomment}

\begin{haskell}
data FBMachine s a = FBM { runFBM :: s -> ((a, s), FBMachine s a) }

unfoldFBM :: FBMachine s a -> s -> Stream a
unfoldFBM fbm s0 = h <: (unfoldFBM cont s1)
		where ((h, s1), cont) = runFBM fbm s0

unfold :: (s -> (a, s)) -> s -> Stream a
unfold f s0 = h <: (unfold f s1)
		where (h, s1) = f s0
\end{haskell}

The key difference is that the State-monster can contain many \emph{different} functions that produce new states and outputs. This gives a richer language by which to calculate outputs, where each function defines what the next function should be, depending on an input state. This makes them analogous to state machines, where $S$ is the set of functions the monster contains, and $\Sigma$ is the state values given as inputs.

Most of what can be said about State-monsters was said in the reader monad section, since the type of feedback machines $\mathsf{FBMachine}(S,\Sigma,\Delta)$ is equivalent to $\mathsf{Mealy}(S,\Sigma,\Sigma \times \Delta)$. The main difference is in how the stream is traversed - the state monad's join operation threads the state though each nested computation, whereas a $\reader$-monster can't do this with the reader monad operations alone.

\subsection{$\io$ monad}

A monadic stream of $\io$ actions corresponds to a non-terminating process. Due to the nature of the $\io$ monad, these operate quite differently to other monsters.

One interesting property is that an $\io$-monster is the only kind of non-well founded (infinite) monster, as far as we know, where collapsing the whole stream into one monadic action is possible and meaningful. This is done using the \verb+runProcess+ family of functions:

\begin{haskell}
type Process a = MonStr IO a

runProcess :: Process a -> IO [a]
runProcess (MCons s) = do (a,s') <- s
                          as     <- runProcess s'
                          return (a:as)
\end{haskell}

This is because $\io$ actions can continuously interact with the outside world, and call other functions, \emph{without} having to terminate. The $\io$ action produced by collapsing an $\io$-monster is the (possibly infinite) sequence of all $\io$ operations in the stream. \\

Given a process (an $\io$-monster), you may want to use some of the output values in another computation. To access these values, you need to run the $\io$ actions that the monster consists of. However, since the first action will never return (as all of the subsequent actions are nested inside), this is not immediately possible with strict $\io$. Instead, you have to use lazy $\io$, so that values that are needed outside of the process are only calculated as required. This requires \verb+unsafeInterleaveIO+ \cite{unsafe_io}, which defers execution of $\io$ actions until evaluation of their result is forced by another computation. 

In these functions, we are trying to output the first value computed by running a process \verb+proc0+:

\begin{haskell}
proc0 :: Process a

run0 :: IO ()
run0 = do as <- runProcess proc0
          putStrLn $ show (as !! 0)

run1 :: IO ()
run1 = do as <- unsafeRunProcess proc0
          putStrLn $ show (as !! 0)
\end{haskell}

Here, \verb+run0+ will run the process \verb+proc0+, and the line that prints the first element will never be reached since \verb+runProcess proc0+ doesn't terminate.

In \verb+run1+, we use \verb+unsafeRunProcess+, which is defined as:

\begin{haskell}
unsafeRunProcess :: Process a -> IO [a]
unsafeRunProcess (MCons s) = 
	do (a,s') <- s
	   as     <- unsafeInterleaveIO (unsafeRunProcess s')
	   return (a:as)
\end{haskell}

Each of the $\io$ actions in the process are run with \verb+unsafeInterleaveIO+, allowing the process to return `early', and the \verb+putStrLn $ show (as !! 0)+ statement to be reached. This print statement forces the first $\io$ action in the process to run, since the first element of \verb+as+ is needed (\verb+as !! 0+ returns the first element of the accumulated values from running the process).

Using \verb+unsafeInterleaveIO+ introduces concurrency problems into otherwise relatively pure Haskell programs. For some $\io$-monsters, such as those that simply output values to the terminal, this is not a big issue, but more complicated ones may cause problems (for example ones that read and write to a file). \\
\vcomm{We could then claim that this is an advantage of IO-monsters: using a monster directly without collapsing it into a single IO action will allow us to sequence it lazily without using unsafeInterleaveIO.
I guess this is what is done below, but I don't fully understand it myself.
}


This problem informed the development of a more general operation on monadic streams, \verb+interleaveReadM+, which allows for interleaving two monsters, where each element in the second depends on elements in the first.

\begin{haskell}
interleaveReadM :: Monad m => MonStr m a -> MonStr (ReaderT a m) b 
															-> MonStr m b
interleaveReadM (MCons ma) (MCons f) = MCons $ 
	do (a, ma') <- ma
	   (b, f')  <- runReaderT f a 
	   return (b, interleaveReadM ma' f')
\end{haskell}

This is useful for sequencing $\io$-monsters, without resorting to possibly unsafe methods. It relies on the \verb+ReaderT+ monad transformer, monad transformers being something we haven't yet discussed. They are essentially a method of stacking different monads to combine their effects. Here, we are using the \verb+ReaderT a IO+ monad, which represents an $\io$ action that is computed from some environment of type \verb+a+.

Shown below is an example of combining two processes with this operation.

\begin{haskell}
inputProc :: Process Char
inputProc = MCons $ do c <- getChar
                       return (c, inputProc)
                       
outputProc :: Show a => MonStr (ReaderT a IO) ()
outputProc = MCons $ do a <- ask 
                        liftIO $ putStrLn (show a)
                        return ((), outputProc)

testProc :: IO ()
testProc = runVoidProcess (interleaveReadM inputProc outputProc)
\end{haskell}

Using this function lets you define dependent processes separately and then combine them afterwards. 
`Dependent' refers to the use of the \verb+ReaderT+ monad transformer: the actions of the dependent process (\verb+outputProc+) are specified by the elements in another process (\verb+inputProc+).

Variations on this function give lots of options for modifying processes. For example, we could write a version of \verb+run1+ from earlier using \verb+insertActReadM+, which inserts a dependent monadic action at a given index.

\begin{haskell}
run1 :: IO [Int]
run1 = runProcess (insertActReadM 0 (putStrLn . show) proc0)
\end{haskell}

This won't terminate, but prints the first element of the $\io$-monster like we wanted, by inserting the print function directly into the process.

To stop this process (without resorting to \verb+unsafeInterleaveIO+), we define a variation of \verb+runProcess+ which stops unfolding the $\io$-monster when a given predicate is true:

\begin{haskell}
stopAtPred :: (a -> Bool) -> Process a -> IO a
stopAtPred p (MCons s) = do (a, s') <- s
                            if p a then (return a) else (stopAtPred p s')
\end{haskell}
 
As a side-note, \verb+runProcess+ can now be written as:
 
 \begin{haskell}
runProcess :: Process a -> IO a
runProcess = stopAtPred (\_ -> False)
\end{haskell}

With this, it is now possible to terminate the process given a particular predicate. It is also possible to terminate a process at a particular index in the $\io$-monster, thus giving exactly the behaviour we wanted for \verb+run1+, without resorting to unsafe $\io$ (this example is included in the attached code, as it is a bit more verbose). \\

As demonstrated, $\io$-monsters are tricky, but provide an interesting intensional way of modelling processes, allowing the computations that they consist of to be modified and interleaved after their definition.
 
\subsection{Store comonad}

The store comonad is a pair of a state (called the store) and a function to extract a value from that state:

\begin{haskell}
data Store s a = Store (s -> a) s
\end{haskell}

A Store-comonster, a monadic stream using the store comonad, is a stream where extracting from the store gives a value, and a \emph{new store}. This encapsulates the idea of an environment that can be used to modify itself. Cellular automata fit this description perfectly, where the state of the environment is calculated from the previous using a rule. 

The comonadic interpretation of cellular automata is a standard introduction to the motivation behind and uses of comonads. The difference when using (co)monadic streams is that a Store-comonster already `contains' every future state of its environment, so extra functions can be applied lazily to these before they are actually evaluated. By representing the structure intensionally rather than extensionally, you gain an extra level of control.

To show this, there is an implementation of Conway's game of life included in the demonstration code. This demo uses a kind of Zipper comonad instead of store for some more efficiency, but the type used is isomorphic to \verb+Store (Int,Int) Bool+.
