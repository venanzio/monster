\section{Introduction}

A monadic stream is a sequence of values in which every element is obtained by triggering a monadic action.
If $\sigma$ is such a stream, it will consist of an action for a certain monad $M$ that, when executed, will return a head (first element) and a tail (continuation of the stream).
This process can be continued in a non-well-founded way: streams constitute a coinductive type.

Formally the type of streams over a monad $M$ (let's call them {\em $M$-monsters}) with elements of type $A$ is defined, with an Agda-like notation \cite{agda}, as:

$$
\begin{array}[t]{l}
\codata\;
\stream{M,A}:\set\\
\quad \mcons_M: M\,(A\times \stream{M,A})\rightarrow\stream{M,A}
\end{array}
$$

Categorically, we can see this type as the {\em final coalgebra} of the functor $F_M\,X = M\,(A\times X)$.
The final coalgebra does not necessarily exist for every $M$, 
but it does for most of the commonly used monads, specifically for those that are container functors \cite{AAG:2005}.

The definition of $M$-monsters is very close to that of {\em cofree (or iterative) comonad}, which can be seen as the type of $M$-monsters with a pure 
leading value \cite{AAMV:2003,CUV:2006}.

The monadic streams definition is a type operator that maps a type $A$ to the type of $M$-monsters with elements of type $A$; we may indicate the operator by $\stream{M}$ and the type by the slightly different notation $(\stream{M}\,A)$.
This notation will be useful when we prove properties of the operator, for example that it is an applicative functor.

Instantiating $M$ with some of the most well-known monads leads to versions of known data types or to interesting new constructs.

If we instantiate $M$ with the identity monad, we obtain the type of pure streams.
Its usual definition is the following:

$$
\begin{array}[t]{l}
\codata\;
\stream{A}:\set\\
\quad (\scons): \nat\rightarrow \stream{\nat} \rightarrow\stream{\nat}.
\end{array}
$$

An element of $\stream{A}$ is an infinite sequence of elements of $A$: $a_0 \scons a_1\scons a_2\scons \cdots$.

If we instantiate $M$ with the $\maybe$ monad we obtain the type $\stream{\maybe,A}$, equivalent to the type of lazy lists $\lst{A}$.
The $\maybe$ monad is a functor that adds an extra element to the argument type:
$\maybe\,X$ contains copies of each element $x:X$, denoted by $\just\,x$, plus a {\em empty} element $\nothing$.
So $\maybe\,X \cong X+1$.
The single constructor $\mcons_\maybe: \maybe\,(A\times \stream{\maybe,A})\rightarrow\stream{\maybe,A}$ is equivalent to two constructors (for $\nothing$ and $\just$):

$$
\begin{array}[t]{l}
\codata\;
\lst{A}:\set\\
\quad (\scons): A\times \lst{A}\rightarrow\lst{A}\\
\quad \nil: \lst{A}.
\end{array}
$$

This means that an element of $\lst{A}$ is either an empty sequence $\nil$ or a non-empty sequence $a\scons \sigma$ where $a:A$ and $\sigma$ is recursively an element of $\lst{A}$.
Since this is a coinductive type, the constructor $(\scons)$ can be applied an infinite number of times.
Therefore $\lst{A}$ is the type of finite and infinite sequences.

Another example is when the underlying monad is $M = \lstsym $ itself.
In this case each entry in the stream is a list of pairs of heads and tails.
This is equivalent to trees of arbitrary branching degrees (finite branches if we use only finite lists, but also countably infinite branches if we use lazy lists).
Since the type is coinductive, the trees can be non-well-founded, that is, they may be infinitely deep.

If we choose $M$ to be the {\em state transformer} monad, $M$-monsters are state machines that, at every step, produce an output value that depends on an underlying state and change the state itself.
The state transformer monad is defined as $\state_S\,A = S\rightarrow A\times S$, where $S$ is the type of underlying states.
We use the notation $\stmon{S}\,A = \stream{\state_S}\,A$ to denote state monsters.
They will be useful to construct the counterexample to the monad laws in Section \ref{sec:monad}.

Finally, in Haskell user interaction and other system effects are encapsulated in the $\io$ monad. An $\io$ action produces a value that possibly depends and triggers effects. $\io$-monsters are interactive processes that continuously interface with the external world.

A companion paper that we will publish later will present the Haskell monster library that we developed and describe several of its applications.

It is important to make two observations about the underlying ``monad'' $M$.

First, $M$ does not need to be a monad for the definition to make sense. 
In fact we will obtain several interesting results when $M$ satisfies weaker conditions, for example being an applicative functor.
So we will take $M$ to be any type operator (but see second observation) and we will explicitly state what properties we assume about it.
The most important instances are monads and it is convenient to use the facilities of monadic notation in programming and monad theory in reasoning.

The second observation is that it is not guaranteed in general that the $\codata$ type is well-defined.
Haskell will accept the definition when $M$ is any operator, but mathematically the type is well defined only when $F_M\,X = M\,(A\times X)$ is a functor with a final coalgebra.
As stated above, this is the case if $M$ is a container functor and we assume that it is for the rest of the article.

Functions with $M$-monsters as codomain can be defined by explicitely giving a coalgebra.
In practical programming it is more convenient to define function by {\em corecursive equations} that satisfy the property of {\em guardedness by constructors}:
a corecursive equations is accepted if the right-hand side is a term with a constructor on top and recursive calls occurring only as direct arguments of that constructor.

As an example, here is a function that generates a state machine from a natural number:

$$
\begin{array}{l}
\fromstepstr : \nat \rightarrow \stmon{\nat}\,\nat \\
\fromstepstr\, n = \mcons\, (\lambda s. \pair{\pair{s}{\fromstepstr\, (n + 1)}}{s + n})
\end{array}
$$

Given an input $n$, this defines a state-monster that reads the current state $s$, returns its value as output, updates the state to $s+n$, and continues recursively by calling itself on input $n+1$.
In the end it will produce the infinte sequence $s \scons s+n \scons s+n+1 \scons s+n+2 \scons \cdots$.
The justification of the soundness of the definition is in the fact that the recursive call $\fromstepstr\, (n + 1)$ occurs under the guard of the constructor $\mcons $ and is used to generate the recursive sub-stream of the main output value.
The fact that the input $n+1$ is larger than the original input $n$ is irrelevant to corecursive definitions (contrary to what happens in inductive definitions): what matter is that the first element of the stream and the new state are produced before the call is executed.

Another example is the function from monsters to monsters that increases every element by one.
This is a function polymorphic on $M$ that only assumes that $M$ is a functor.

$$
\begin{array}{l}
\incr : \stream{M}\,\nat \rightarrow \stream{M}\,\nat\\
\incr\,(\mcons\,m) = \mcons\,(M\,(\lambda \pair{n}{\sigma}. \pair{n+1}{\incr\,\sigma})\,m)
\end{array}
$$

Here the pattern-matching variable $m$ has type $M\,(\nat\times \stream{M}\,\nat)$.
We map onto it the function that increases the output value by one and recursively applies $\incr$ to the tail.
This recursive occurrence is justified because it is guarded by $\mcons$ and mapped to the direct subterms inside the monad action.
By the way,
after we prove that $\stream{M}$ is a functor whenever $M$ is in Section \ref{sec:functor}, and that it is applicative whenever $M$ is in Section \ref{sec:applicative},
the definition can be simplified and made more intuitive in the following ways, respectively:

$$
\incr\,\sigma = \stream{M}\,(+1)\,\sigma
\qquad
\incr\,\sigma = \apure\,(+1) \appl \sigma
$$


See previous survey work \cite{capretta:2011} for an overview of the theory of final coalgebras, coinductive types, and corecursive definitions.










%%%%%%%%%%%%%%%%  COINDUCTION PRINCIPLE

Inductive types comply with an {\em induction principle}, which states that we can prove statements about them by bottom-up recursion on their structure.
Dually, coinductive types comply with a {\em coinduction principle}, which states that we can prove equalities of their elements by top-down {\em co-recursion} on their structure.

Let us illustrate the idea with the example of pure streams:
Suppose that we want to prove that two streams $\sigma_0$ and $\sigma_1$ are equal.
Since streams are infinite sequences of elements, that will require proving equality of their entries in corresponding positions: if $\sigma_0 = a_0\scons a_1\scons a_2\scons \cdots$ and  $\sigma_1 = b_0\scons b_1\scons b_2\scons \cdots$, we must prove $a_0 = b_0$, $a_1 = b_1$, $a_2=b_2$, and so on (we assume equality on streams is extensional).
This requires an infinite sequence of equalities to prove, and clearly we cannot produce all of these explicitly.
However, the proof of equality can be seen itself as a stream of proofs of equalities of each pair of elements in the same positions.
We can use the same principle of guarded recursion to generate all the proofs: we recursively assume that we can prove the equality of the tail streams and we only need to give explicitly the equality of the heads.

More specifically, we often need to prove that two \emph{functions} produce equal streams.
Suppose $f,g:X \rightarrow \stream{A}$ are the two functions, and they are both defined by guarded recursion (that is, defined by coalgebras):
$$
f\,x = h_f(x) \scons f\,(t_f(x)) \qquad g\,x = h_g(x) \scons f\,(t_g(x))
$$
So $f$ is defined by the coalgebra $\langle h_f, t_f\rangle : X \rightarrow A\times X$ and $g$ is defined by the coalgebra $\langle h_g, t_g\rangle : X \rightarrow A\times X$.
We want to prove that the two functions are extensionally equal; let us give a name to the proof of that statement:
$$
H: \forall x:X, f\,x = g\,x
$$
To prove this statement we can: first, show directly that the heads must be equal, $p_h:h_f(x) = h_g(x)$; then invoke the {\em coinduction hypothesis} $H$ for the tails.
However, since the recursive calls are applied to potentially different elements of $X$ ($t_f(x)$ and $t_g(x)$), we can't apply $H$ directly.

We can get around this problem by generalizing our goal: instead of proving that the functions give the same result when applied to the same input, we aim for the stronger statement that they give the same result when applied to inputs related by a certain binary relation $\bisim$:
$$
H: \forall x\, y:X, x\bisim y \rightarrow f\,x = g\,y.
$$
If $\bisim$ is reflexive, this will imply our original goal.

Two other properties are necessary to make things work.
First, we need the equality of the heads of the output: if $x\bisim y$, then $h_f(x) = h_g(y)$.
Second, we need that $\bisim$ is preseved under taking the tail functions for $f$ and $g$: if $x\bisim y$, then $t_f(x) \bisim t_g(y)$; so that we can now apply $H$ to obtain that $f(t_f(x)) = g(t_g(y))$.

A relation $\bisim$ satisfying these two properties is called a {\em bisimulation} between the coalgebras $\langle h_f, t_f\rangle$ and $\langle h_g, t_g\rangle$.
The principle of coinduction states that bisimilar elements generate equal streams.

This notion can be generalized to coalgebras of any functor $F$.
To formulate it we need to define the {\em lifting} of a relation by the functor $F$: if $\bisim$ is a relation on a type $X$, we want to lift it to a relation $\bslift{F}$ on $F\,X$.
A simple way to do it is to apply $F$ to the set-theoretic characterization of the relation: the set of all pairs that satisfy it, $R = \{ \langle x, y \rangle \mid x \bisim y\}$.
We can then use the functorial lift of the projections $\pi_0, \pi_1:R \rightarrow X$ to say that two elements $u,w:F\,X$ are related, $u \bslift{F} w$, if there exists an element $s:F\,R$ such that $F\,\pi_0\,s = u$ and $F\,\pi_1\,s = w$.
This can be further generalized by allowing $R$ to be any type: a collection of {\em receipts} that certify that pairs of elements are related.
In category theory, this notion is called a {\em span}.

\begin{definition}\label{def:span}
Let $A$ be a type; a {\em span} on $A$ is a triple $\langle R,r_1,r_2\rangle$ where $R$ is a type and $r_1, r_2$ are functions $R\rightarrow A$.
If $F$ is a functor, the {\em lifting of the span $R$ by $F$} is the span $\langle F\,R,F\,r_1,F\,r_2\rangle$ on $F\,A$.
\end{definition}

\begin{definition}\label{def:bisimulation}
Let $\langle A,\alpha \rangle$ be a coalgebra.
A span $\langle R,r_1,r_2\rangle$ is a {\em bisimulation} if there exists a morphism $\rho:R\rightarrow FR$ such that both $r_1$ and $r_2$ are coalgebra morphisms from $\langle R,\rho\rangle$ to $\langle A,\alpha\rangle$:

DELETED DIAGRAM

(The diagram here is used only to declare the type of the morphisms: we don't assume that it commutes.
The only equalities are the ones stated on the right.)
\end{definition}

The idea is that a relation is a bisimulation if, whenever two elements of $A$ are related by it, then their images through $\alpha$ are related by the lifting.
If $F$ is a container and you think of $\alpha$ as giving the structure of an element this says: if two elements are related, then they must have the same shape, with components in corresponding positions also related.
This notion of bisimulation was first introduced by Park \cite{park:1981} and Milner \cite{milner:1980} as a way of reasoning about processes.
Similar concepts were developed earlier in other fields and substantial previous work prepared the background for its appearance.
The survey article by Sangiorgi \cite{sangiorgi:2009} tells the history of the idea.  
Aczel \cite{aczel:1988} adopted it as the appropriate notion of equality for non-well-founded sets.
There are subtle differences between several notions of bisimulation that are not equivalent in full generality: recent work by Staton \cite{staton:2009} investigates their correlations. \\
On $\stream{A}$ a bisimulation is a binary relation $\bisim$ such that:
$$
\forall \sigma_1, \sigma_2:\stream{A}. \sigma_1 \bisim \sigma_2 \Rightarrow
\hd{\sigma_1} =\hd{\sigma_2} \land
\tl{\sigma_1}\bisim \tl{\sigma_2}
$$
Notice that $\sigma_1\bisim \sigma_2$ guarantees that corresponding elements in the infinite sequences defined by $\sigma_1$ and $\sigma_2$ are equal, that is, $\sigma_1$ and $\sigma_2$ are extensionally equal.
In fact, by repeatedly applying the above property, we have that $\heads{\sigma_1}{n} = \heads{\sigma_2}{n}$ for every $n$. 

\begin{definition}\label{def:coinduction}
A coalgebra $\langle A,\alpha\rangle$ is said to satisfy the {\em coinduction principle} if for every bisimulation $\langle R,r_1,r_2\rangle$ we have that $r_1=r_2$ extensionally.
That is, all pairs related by the bisimulation are equal.
\end{definition}

Intuitively, the coinduction principle states that the elements of $A$ are completely characterised by their structure, which can be infinite.
There is a well-known connection between finality of a coalgebra and the coinduction principle.

\begin{theorem}\label{th:coinduction}
Final coalgebras satisfy the coinduction principle.
\end{theorem}

In practice the coinduction principle is applied by a corecursive proof that can invoke the statement to be proved under certain structural restrictions.
When proving the equality of two given terms, we can appeal to the statement that we want to prove, as a {\em coinduction hypothesis}, as long as it is {\em guarded by constructors} in the sense that it is only deployed to prove the equality of direct components of the given terms.

As an example, let us use the coinduction principle to prove that the two versions of the function $\sfr$ defined in Section \ref{sec:mstreams} are equal,
that is, $\sfr\,n = \sfp\,n$ for all natural numbers $n$.
We will first need a lemma about $\sfp$, basically saying that it satisfies the tail equaltion of $\sfr$, which we also prove by coinduction.
\begin{lemma}\label{lemma:sfp}
For all natural numbers $n$, $\tail\,(\sfp\,n) = \sfp\,(n+1)$.
\end{lemma}
\begin{proof}
We unfold the two sides of the equality, simplifying according to the definitions of $\sfp$, $\oplus$, and $\bar{1}$:
$$
\begin{array}{r@{{}={}}l}
\tail\,(\sfp\,n)
& \tail\,(n \scons (\sfp\,n \oplus \bar{1}))\\
& \sfp\,n \oplus \bar{1}\\
& (\head\,(\sfp\,n) +1) \scons (\tail\,(\sfp\,n)  \oplus \bar{1})\\
& (n+1) \scons (\tail\,(\sfp\,n)  \oplus \bar{1})\\
\sfp\,(n+1)
& (n+1) \scons (\sfp\,(n+1) \oplus \bar{1})
\end{array}
$$
The two expressions have the same head, $n+1$, so we just need to prove that the tails are equal.
For this we can simply invoke the {\em coinductive hypothesis}, that is, apply the statement of the lemma $\tail\,(\sfp\,n) = \sfp\,(n+1)$.
\end{proof}

To readers unfamiliar with this style of reasoning, this proof may seem hopelessly circular: we invoke the statement that we're trying to prove in its own proof.
However, the circular appeal to the statement is restricted to proving equality of the tails: it is guarded by constructors. 
This constraint on the structure of the proof makes it ameanable to be reformulated more rigorously in the form of a bisimulation.
(Strictly speaking, the coinductive hypothesis is applied to terms that are arguments of $\oplus$, so it is not technically guarded; but as in the case of the definition of $\sfp$, this lax use can be readily justified.)

We can now use the lemma to prove that both versions of the function are equal.
\begin{theorem}
For all natural numbers $n$, $\sfr\,n = \sfp\,n$.
\end{theorem}
\begin{proof}
We unfold the left-hand side of the equation, then use the coinductive hypothesis and the previous lemma to reduce it to the right-hand side.
$$
\begin{array}{r@{{}={}}ll}
\sfr\,n
& n \scons \sfr\,(n+1)\\
& n \scons \sfp\,(n+1) & \mbox{by Coinductive Hypothesis}\\
& n \scons \tail\,(\sfp\,n) & \mbox{by Lemma \ref{lemma:sfp}}\\
& \sfp\,n
\end{array}
$$
The last step is justified because $n = \head\,(\sfp\,n)$ by definition and trivially any stream $\sigma$ is equal to $\head\,\sigma \scons \tail\,\sigma$.
\end{proof}

When working with monadic streams, we can similarly use proof by coinduction by invoking as coinductive hypothesis the statement we are proving.
Uses of the coinductive hypothesis are justified if they are applied to the {\em sub-components} of the stream.

We will illustrate this when we prove statements about some of the instantiations of monadic streams.
For example, when using $\lstsym$-monsters (monadic streams where the underlying monad $M$ is $\lstsym$), we are allowed to apply the coinductive hypothesis to the tails of all elements in the list.
